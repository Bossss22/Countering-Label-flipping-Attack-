import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, confusion_matrix
from torch.utils.data import Dataset, DataLoader, Subset
from imblearn.over_sampling import SMOTE
import random
from collections import defaultdict
import matplotlib.pyplot as plt
import os
import warnings
import json
from datetime import datetime

warnings.filterwarnings('ignore')

def set_full_determinism():
    """Set all random seeds for complete reproducibility"""
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
        torch.cuda.manual_seed_all(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = '42'
    
    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % 2**32
        np.random.seed(worker_seed)
        random.seed(worker_seed)
    
    g = torch.Generator()
    g.manual_seed(42)
    return g, seed_worker

data_loader_generator, seed_worker_fn = set_full_determinism()
device = torch.device('cpu')

# ===== DATASET CLASS (SAME AS ORIGINAL) =====
class V2GDataset(Dataset):
    def __init__(self, csv_path):
        dataset_rng = random.Random(42)
        np_rng = np.random.RandomState(42)
        
        self.data = pd.read_csv(csv_path)
        print(f"Dataset loaded: {len(self.data)} records, columns: {list(self.data.columns)}")

        if 'timestamp' in self.data.columns:
            self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])
            self.data['hour'] = self.data['timestamp'].dt.hour
            self.data['day_of_week'] = self.data['timestamp'].dt.dayofweek
            self.data['is_weekend'] = (self.data['day_of_week'] >= 5).astype(int)
        else:
            self.data['hour'] = 0
            self.data['day_of_week'] = 0
            self.data['is_weekend'] = 0

        eps = 1e-6
        self.data['charge_discharge_ratio'] = self.data['current_charge_kWh'] / (self.data['discharge_rate_kW'] + eps)
        self.data['energy_hour_interaction'] = self.data['energy_requested_kWh'] * self.data['hour']
        self.data['charge_capacity_ratio'] = self.data['current_charge_kWh'] / (self.data['battery_capacity_kWh'] + eps)
        self.data['efficiency_estimate'] = self.data['energy_requested_kWh'] / (self.data['discharge_rate_kW'] * 0.25 + eps)

        if 'label' not in self.data.columns:
            raise ValueError("Dataset must contain 'label' column")
        
        self.data['label'] = self.data['label'].astype(str).str.lower().str.strip()
        valid_labels = ['honest', 'adversarial']
        self.data = self.data[self.data['label'].isin(valid_labels)]
        
        if self.data.empty:
            raise ValueError("No valid labels found. Expected 'honest' or 'adversarial'")

        self.data = self.data.dropna(subset=['label'])

        if 'participant_id' not in self.data.columns:
            raise ValueError("Dataset must contain 'participant_id' column")

        self.participant_ground_truth = {}
        for pid in self.data['participant_id'].unique():
            pid_data = self.data[self.data['participant_id'] == pid]
            label_counts = pid_data['label'].value_counts()
            majority_label = label_counts.idxmax()
            self.participant_ground_truth[int(pid)] = majority_label
        
        print("\n=== GROUND TRUTH PARTICIPANT LABELS ===")
        honest_pids = [pid for pid, label in self.participant_ground_truth.items() if label == 'honest']
        adv_pids = [pid for pid, label in self.participant_ground_truth.items() if label == 'adversarial']
        print(f"Honest: {sorted(honest_pids)} (n={len(honest_pids)})")
        print(f"Adversarial: {sorted(adv_pids)} (n={len(adv_pids)})")

        feature_cols = [
            'battery_capacity_kWh', 'current_charge_kWh', 'discharge_rate_kW',
            'energy_requested_kWh', 'hour', 'charge_discharge_ratio',
            'energy_hour_interaction', 'charge_capacity_ratio',
            'efficiency_estimate', 'day_of_week', 'is_weekend'
        ]
        
        for col in feature_cols:
            if col not in self.data.columns:
                raise ValueError(f"Missing column: {col}")

        self.features = self.data[feature_cols].values
        self.labels = self.data['label'].values
        self.participant_ids = self.data['participant_id'].values
        self.original_indices = np.arange(len(self.data))

        self.label_encoder = LabelEncoder()
        original_labels = self.label_encoder.fit_transform(self.labels)

        print("Applying data augmentation...")
        np_rng = np.random.RandomState(42)
        X_aug = self.features.copy()
        noise = np_rng.normal(0, 0.1, size=X_aug.shape)
        X_aug = X_aug + noise
        
        for i in range(X_aug.shape[0]):
            shift_amount = (i % 3) - 1
            if shift_amount != 0:
                X_aug[i] = np.roll(X_aug[i], shift_amount)
        
        scale_factors = np_rng.uniform(0.9, 1.1, size=(X_aug.shape[0], 1))
        X_aug = X_aug * scale_factors
        
        y_aug = self.labels
        self.features = np.vstack([self.features, X_aug])
        self.labels = np.concatenate([self.labels, y_aug])
        self.participant_ids = np.concatenate([self.participant_ids, self.participant_ids])
        self.original_indices = np.concatenate([self.original_indices, self.original_indices])

        smote = SMOTE(sampling_strategy='auto', k_neighbors=3, random_state=42)
        self.features, self.labels = smote.fit_resample(self.features, self.labels)
        self.labels = self.label_encoder.transform(self.labels)

        for pid in np.unique(self.participant_ids):
            idx = np.where(self.participant_ids == pid)[0]
            if len(idx) < 10:
                unique_labels = np.unique(self.labels[idx])
                if len(unique_labels) > 1:
                    extra_samples, extra_labels = smote.fit_resample(self.features[idx], self.labels[idx])
                    extra_samples = extra_samples[:10 - len(idx)]
                    extra_labels = extra_labels[:10 - len(idx)]
                else:
                    num_needed = 10 - len(idx)
                    participant_rng = np.random.RandomState(42 + int(pid))
                    indices_to_duplicate = participant_rng.choice(idx, size=num_needed, replace=True)
                    extra_samples = self.features[indices_to_duplicate]
                    extra_labels = self.labels[indices_to_duplicate]

                self.features = np.vstack([self.features, extra_samples])
                self.labels = np.concatenate([self.labels, extra_labels])
                self.participant_ids = np.concatenate([self.participant_ids, [pid] * len(extra_samples)])
                self.original_indices = np.concatenate([self.original_indices, [self.original_indices[idx[0]]] * len(extra_samples)])

        self.scaler = StandardScaler()
        self.features = self.scaler.fit_transform(self.features)

        self.num_features = self.features.shape[1]
        self.num_classes = len(self.label_encoder.classes_)

        self.features = torch.tensor(self.features, dtype=torch.float32)
        self.labels = torch.tensor(self.labels, dtype=torch.long)

        self.participant_indices = defaultdict(list)
        for idx, pid in enumerate(self.participant_ids):
            self.participant_indices[pid].append(idx)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

    def get_participant_ground_truth(self, participant_id):
        return self.participant_ground_truth.get(int(participant_id), None)

    def get_participant_indices(self, participant_id):
        return self.participant_indices.get(participant_id, [])

# ===== MODEL ARCHITECTURE =====
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.98, gamma=4.0):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, outputs, targets):
        nll_loss = nn.NLLLoss(reduction='none')(outputs, targets)
        pt = torch.exp(-nll_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * nll_loss
        return focal_loss.mean()

class V2GClassifier(nn.Module):
    def __init__(self, input_size, hidden_size=512, num_classes=2, dropout=0.35):
        super(V2GClassifier, self).__init__()
        torch.manual_seed(42)
        self.input_layer = nn.Linear(input_size, hidden_size)
        self.norm1 = nn.LayerNorm(hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.hidden_layer1 = nn.Linear(hidden_size, hidden_size // 2)
        self.norm2 = nn.LayerNorm(hidden_size // 2)
        self.hidden_layer2 = nn.Linear(hidden_size // 2, hidden_size // 4)
        self.norm3 = nn.LayerNorm(hidden_size // 4)
        self.output_layer = nn.Linear(hidden_size // 4, num_classes)
        self.log_softmax = nn.LogSoftmax(dim=1)
        self._reset_parameters()
        
    def _reset_parameters(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
                    
    def forward(self, x):
        x = self.input_layer(x)
        x = self.norm1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.hidden_layer1(x)
        x = self.norm2(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.hidden_layer2(x)
        x = self.norm3(x)
        x = self.relu(x)
        x = self.output_layer(x)
        return self.log_softmax(x)

# ===== FEDAVG SERVER =====
class FedAvgServer:
    def __init__(self, model, num_honest, num_adversarial, device='cpu', dataset=None):
        self.model = model.to(device)
        self.device = device
        self.num_honest = num_honest
        self.num_adversarial = num_adversarial
        self.dataset = dataset
        self.current_round = 0
        self.participants = {}
        self.ground_truth_types = {}
        
        # Metrics tracking
        self.accuracy_history = []
        self.precision_history = []
        self.recall_history = []
        self.f1_history = []
        self.loss_history = []
        self.confusion_matrices = []
        
        self.active_participants = []
        self.honest_removed_count = 0
        self.adversarial_removed_count = 0
        
        self._initialize_participants(dataset)
    
    def _initialize_participants(self, dataset):
        available_pids = list(set(dataset.participant_ids))
        
        honest_ground_truth = []
        adversarial_ground_truth = []
        
        for pid in available_pids:
            ground_truth = dataset.get_participant_ground_truth(pid)
            if ground_truth == 'honest':
                honest_ground_truth.append(pid)
            elif ground_truth == 'adversarial':
                adversarial_ground_truth.append(pid)
        
        print(f"\nFedAvg - Dataset composition:")
        print(f"  {len(honest_ground_truth)} honest: {sorted(honest_ground_truth)}")
        print(f"  {len(adversarial_ground_truth)} adversarial: {sorted(adversarial_ground_truth)}")
        
        if len(honest_ground_truth) < self.num_honest:
            print(f"Warning: Only {len(honest_ground_truth)} honest available, adjusting from {self.num_honest}")
            self.num_honest = len(honest_ground_truth)
        
        if len(adversarial_ground_truth) < self.num_adversarial:
            print(f"Warning: Only {len(adversarial_ground_truth)} adversarial available, adjusting from {self.num_adversarial}")
            self.num_adversarial = len(adversarial_ground_truth)
        
        rng = random.Random(42)
        honest_sorted = sorted(honest_ground_truth)
        adversarial_sorted = sorted(adversarial_ground_truth)
        rng.shuffle(honest_sorted)
        rng.shuffle(adversarial_sorted)
        
        honest_ids = honest_sorted[:self.num_honest]
        adversarial_ids = adversarial_sorted[:self.num_adversarial]

        def _new_participant(pid):
            torch.manual_seed(42 + pid)
            model = V2GClassifier(self.model.input_layer.in_features, 
                                num_classes=self.model.output_layer.out_features).to(self.device)
            
            return {
                'model': model,
                'type': None,
                'data': None,
                'active': True,
                'data_size': 0,
                'train_loss': 0.0,
                'local_accuracy': 0.0
            }

        for idx in honest_ids:
            p = _new_participant(idx)
            p['type'] = 'honest'
            self.participants[idx] = p
            self.ground_truth_types[idx] = dataset.get_participant_ground_truth(idx)

        for idx in adversarial_ids:
            p = _new_participant(idx)
            p['type'] = 'adversarial'
            self.participants[idx] = p
            self.ground_truth_types[idx] = dataset.get_participant_ground_truth(idx)

        print(f"\nFedAvg Server initialized: {self.num_honest} honest, {self.num_adversarial} adversarial")
        self.active_participants = list(self.participants.keys())
    
    def select_participants(self, fraction=0.5):
        rng = random.Random(42 + self.current_round)
        num_to_select = max(1, int(len(self.active_participants) * fraction))
        selected = rng.sample(self.active_participants, num_to_select)
        return selected
    
    def federated_averaging(self, participant_updates):
        if not participant_updates:
            return
        
        total_data_size = sum(self.participants[pid]['data_size'] for pid in participant_updates)
        
        averaged_params = {}
        
        with torch.no_grad():
            for pid in participant_updates:
                participant_model = self.participants[pid]['model']
                weight = self.participants[pid]['data_size'] / total_data_size
                
                for name, param in participant_model.named_parameters():
                    if pid == participant_updates[0]:
                        averaged_params[name] = param.data.clone() * weight
                    else:
                        averaged_params[name] += param.data.clone() * weight
        
        for name, param in self.model.named_parameters():
            param.data.copy_(averaged_params[name])
    
    def evaluate_model(self, val_loader, return_predictions=False):
        """Evaluate model with full metrics"""
        self.model.eval()
        all_preds = []
        all_labels = []
        all_probs = []
        total_loss = 0.0
        
        criterion = FocalLoss(alpha=0.98, gamma=4.0)
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = self.model(inputs)
                
                # Calculate loss
                loss = criterion(outputs, labels)
                total_loss += loss.item()
                
                # Get predictions
                probs = torch.exp(outputs)
                _, predicted = torch.max(outputs, 1)
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        # Calculate all metrics
        accuracy = accuracy_score(all_labels, all_preds)
        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)
        recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)
        f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)
        
        # Calculate confusion matrix
        cm = confusion_matrix(all_labels, all_preds)
        
        # Calculate AUC-ROC if we have probabilities
        if len(np.unique(all_labels)) > 1:
            try:
                auc_roc = roc_auc_score(all_labels, [p[1] for p in all_probs])
            except:
                auc_roc = 0.0
        else:
            auc_roc = 0.0
        
        avg_loss = total_loss / len(val_loader)
        
        if return_predictions:
            return accuracy, precision, recall, f1, auc_roc, avg_loss, cm, all_preds, all_labels
        else:
            return accuracy, precision, recall, f1, auc_roc, avg_loss, cm
    
    def get_metrics(self):
        active_honest = sum(1 for idx in self.active_participants 
                       if self.ground_truth_types[idx] == 'honest')
        active_adversarial = sum(1 for idx in self.active_participants 
                            if self.ground_truth_types[idx] == 'adversarial')
        total_active = len(self.active_participants)
        adversarial_ratio = active_adversarial / total_active if total_active > 0 else 0
        
        return {
            'active_honest': active_honest,
            'active_adversarial': active_adversarial,
            'total_active': total_active,
            'adversarial_ratio': adversarial_ratio,
            'honest_removed_count': self.honest_removed_count,
            'adversarial_removed_count': self.adversarial_removed_count,
            'current_round': self.current_round
        }

# ===== TRAINING FUNCTION =====
def train_participant_fedavg(participant, dataset, device, current_round, participant_id, is_adversarial=False):
    model = participant['model']
    model.train()
    
    lr = 0.001 * (0.95 ** max(0, current_round - 10))
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = FocalLoss(alpha=0.98, gamma=4.0)
    
    data_loader = participant['data']
    if not data_loader:
        return 0.0, 0.0
    
    adv_rng = random.Random(42 + participant_id + current_round * 1000)
    noise_scale = 0.6 if current_round < 5 else 0.4
    flip_prob = 0.4 if current_round < 5 else 0.3
    
    training_epochs = 3
    total_loss = 0.0
    correct = 0
    total = 0
    
    for epoch in range(training_epochs):
        epoch_loss = 0.0
        for features, labels in data_loader:
            features, labels = features.to(device), labels.to(device)
            
            if is_adversarial:
                if adv_rng.random() < flip_prob:
                    labels = (labels + 1) % dataset.num_classes
                if adv_rng.random() < 0.5:
                    torch.manual_seed(42 + participant_id + current_round * 1000 + epoch)
                    noise = torch.randn_like(features) * noise_scale
                    features = features + noise
            
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()
            
            epoch_loss += loss.item()
            
            # Calculate local accuracy
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        total_loss += epoch_loss / len(data_loader)
    
    avg_loss = total_loss / training_epochs
    local_accuracy = correct / total if total > 0 else 0.0
    
    participant['train_loss'] = avg_loss
    participant['local_accuracy'] = local_accuracy
    
    return avg_loss, local_accuracy

# ===== DATA DISTRIBUTION =====
def distribute_data_to_participants_fedavg(server, dataset, train_indices):
    activated_honest = 0
    activated_adversarial = 0

    for idx in server.participants:
        indices = [i for i in train_indices if i < len(dataset.participant_ids) and dataset.participant_ids[i] == idx]
        num_records = len(indices)
        
        if num_records < 5:
            print(f"Warning: Participant {idx} has only {num_records} records, deactivating")
            server.participants[idx]['active'] = False
            if idx in server.active_participants:
                server.active_participants.remove(idx)
            continue
            
        subset = Subset(dataset, indices)
        batch_size = min(32, max(4, num_records // 2))
        server.participants[idx]['data'] = DataLoader(subset, batch_size=batch_size, shuffle=True, 
                                                     generator=data_loader_generator, worker_init_fn=seed_worker_fn)
        server.participants[idx]['active'] = True
        server.participants[idx]['data_size'] = len(indices)
        
        if server.ground_truth_types[idx] == 'honest':
            activated_honest += 1
        else:
            activated_adversarial += 1

    print(f"Activated: {activated_honest} honest, {activated_adversarial} adversarial")
    return activated_honest, activated_adversarial

# ===== JSON SERIALIZABLE METRICS FUNCTION =====
def convert_to_serializable(obj):
    """Convert numpy arrays and other non-serializable objects to JSON-serializable format"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    else:
        return obj

def calculate_comprehensive_metrics(server, val_loader):
    """Calculate comprehensive metrics for FedAvg with JSON serialization fix"""
    
    # Get final evaluation with all metrics
    accuracy, precision, recall, f1, auc_roc, avg_loss, cm = server.evaluate_model(val_loader)
    
    metrics = server.get_metrics()
    
    # Calculate additional statistics
    if server.accuracy_history:
        final_accuracy = server.accuracy_history[-1]
        max_accuracy = max(server.accuracy_history)
        mean_accuracy = np.mean(server.accuracy_history)
        std_accuracy = np.std(server.accuracy_history)
    else:
        final_accuracy = max_accuracy = mean_accuracy = std_accuracy = 0.0
    
    # Create metrics dictionary with serializable values
    metrics_dict = {
        # Basic metrics
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'auc_roc': float(auc_roc),
        'avg_loss': float(avg_loss),
        
        # Statistical metrics
        'final_accuracy': float(final_accuracy),
        'max_accuracy': float(max_accuracy),
        'mean_accuracy': float(mean_accuracy),
        'std_accuracy': float(std_accuracy),
        
        # History metrics (convert to lists)
        'accuracy_history': [float(x) for x in server.accuracy_history],
        'precision_history': [float(x) for x in server.precision_history],
        'recall_history': [float(x) for x in server.recall_history],
        'f1_history': [float(x) for x in server.f1_history],
        'loss_history': [float(x) for x in server.loss_history],
        
        # Convert confusion matrices to lists
        'confusion_matrices': [cm.tolist() for cm in server.confusion_matrices] if server.confusion_matrices else [],
        
        # Participant metrics
        'active_honest': int(metrics['active_honest']),
        'active_adversarial': int(metrics['active_adversarial']),
        'total_active': int(metrics['total_active']),
        'adversarial_ratio': float(metrics['adversarial_ratio']),
        'honest_removed_count': int(metrics['honest_removed_count']),
        'adversarial_removed_count': int(metrics['adversarial_removed_count']),
        'rounds_completed': int(metrics['current_round']),
        
        # Current confusion matrix as list
        'confusion_matrix': cm.tolist(),
        
        # Timestamp
        'timestamp': datetime.now().isoformat(),
        'scenario': 'Honest Majority' if metrics['active_honest'] > metrics['active_adversarial'] else 'Adversarial Majority'
    }
    
    return metrics_dict

def create_comprehensive_plots(server, metrics, scenario_name, save_dir='.'):
    """Create comprehensive visualization plots"""
    os.makedirs(save_dir, exist_ok=True)
    
    fig = plt.figure(figsize=(20, 16))
    
    # 1. Accuracy over rounds
    ax1 = plt.subplot(3, 3, 1)
    if server.accuracy_history:
        rounds = range(1, len(server.accuracy_history) + 1)
        ax1.plot(rounds, server.accuracy_history, 'b-', linewidth=2, marker='o', markersize=4)
        ax1.axhline(y=np.mean(server.accuracy_history), color='r', linestyle='--', 
                   label=f'Mean: {np.mean(server.accuracy_history):.3f}')
        ax1.set_xlabel('Round')
        ax1.set_ylabel('Accuracy')
        ax1.set_title(f'Accuracy Over Rounds')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
    
    # 2. All metrics over rounds
    ax2 = plt.subplot(3, 3, 2)
    if server.accuracy_history:
        rounds = range(1, len(server.accuracy_history) + 1)
        ax2.plot(rounds, server.accuracy_history, 'b-', label='Accuracy', linewidth=2)
        if server.precision_history:
            ax2.plot(rounds, server.precision_history, 'g-', label='Precision', linewidth=2)
        if server.recall_history:
            ax2.plot(rounds, server.recall_history, 'r-', label='Recall', linewidth=2)
        if server.f1_history:
            ax2.plot(rounds, server.f1_history, 'm-', label='F1', linewidth=2)
        ax2.set_xlabel('Round')
        ax2.set_ylabel('Score')
        ax2.set_title('All Metrics Over Rounds')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
    
    # 3. Confusion Matrix
    ax3 = plt.subplot(3, 3, 3)
    if metrics.get('confusion_matrix'):
        cm = np.array(metrics['confusion_matrix'])
        im = ax3.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
        ax3.set_title(f'Confusion Matrix (Final Round)\nAccuracy: {metrics["accuracy"]:.3f}')
        plt.colorbar(im, ax=ax3)
        classes = ['Honest', 'Adversarial']
        tick_marks = np.arange(len(classes))
        ax3.set_xticks(tick_marks)
        ax3.set_xticklabels(classes)
        ax3.set_yticks(tick_marks)
        ax3.set_yticklabels(classes)
        
        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax3.text(j, i, format(cm[i, j], 'd'), 
                        ha="center", va="center", 
                        color="white" if cm[i, j] > thresh else "black",
                        fontsize=14)
    
    # 4. Precision-Recall tradeoff
    ax4 = plt.subplot(3, 3, 4)
    if server.precision_history and server.recall_history:
        ax4.scatter(server.recall_history, server.precision_history, alpha=0.6, 
                   c=range(len(server.precision_history)), cmap='viridis')
        ax4.set_xlabel('Recall')
        ax4.set_ylabel('Precision')
        ax4.set_title('Precision-Recall Tradeoff')
        ax4.grid(True, alpha=0.3)
    
    # 5. Loss over rounds
    ax5 = plt.subplot(3, 3, 5)
    if server.loss_history:
        rounds = range(1, len(server.loss_history) + 1)
        ax5.plot(rounds, server.loss_history, 'r-', linewidth=2)
        ax5.set_xlabel('Round')
        ax5.set_ylabel('Loss')
        ax5.set_title('Loss Over Rounds')
        ax5.grid(True, alpha=0.3)
    
    # 6. Participant composition
    ax6 = plt.subplot(3, 3, 6)
    labels = ['Honest', 'Adversarial']
    sizes = [metrics.get('active_honest', 0), metrics.get('active_adversarial', 0)]
    colors = ['green', 'red']
    ax6.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    ax6.set_title('Active Participant Composition')
    
    # 7. Metric comparison bar chart
    ax7 = plt.subplot(3, 3, 7)
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    metric_values = [metrics.get('accuracy', 0), metrics.get('precision', 0), 
                    metrics.get('recall', 0), metrics.get('f1_score', 0)]
    colors = ['blue', 'green', 'red', 'purple']
    bars = ax7.bar(metric_names, metric_values, color=colors)
    ax7.set_ylabel('Score')
    ax7.set_title('Final Model Metrics')
    ax7.set_ylim([0, 1])
    for bar, value in zip(bars, metric_values):
        height = bar.get_height()
        ax7.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:.3f}', ha='center', va='bottom')
    
    # 8. Summary text
    ax8 = plt.subplot(3, 3, 8)
    summary_text = f"""
    FedAvg Performance Summary
    ==========================
    Scenario: {scenario_name}
    
    Final Metrics:
    Accuracy:  {metrics.get('accuracy', 0)*100:.2f}%
    Precision: {metrics.get('precision', 0)*100:.2f}%
    Recall:    {metrics.get('recall', 0)*100:.2f}%
    F1-Score:  {metrics.get('f1_score', 0)*100:.2f}%
    AUC-ROC:   {metrics.get('auc_roc', 0)*100:.2f}%
    
    Statistics:
    Max Accuracy: {metrics.get('max_accuracy', 0)*100:.2f}%
    Mean Accuracy: {metrics.get('mean_accuracy', 0)*100:.2f}%
    Std Accuracy: {metrics.get('std_accuracy', 0)*100:.2f}%
    
    Participants:
    Honest: {metrics.get('active_honest', 0)}
    Adversarial: {metrics.get('active_adversarial', 0)}
    Adversarial Ratio: {metrics.get('adversarial_ratio', 0)*100:.1f}%
    
    Rounds: {metrics.get('rounds_completed', 0)}
    """
    ax8.text(0.1, 0.5, summary_text, fontfamily='monospace', fontsize=9, 
             verticalalignment='center', transform=ax8.transAxes)
    ax8.axis('off')
    
    # 9. Metric distribution
    ax9 = plt.subplot(3, 3, 9)
    all_metrics = []
    metric_labels = []
    if server.accuracy_history:
        all_metrics.append(server.accuracy_history)
        metric_labels.append('Accuracy')
    if server.precision_history:
        all_metrics.append(server.precision_history)
        metric_labels.append('Precision')
    if server.recall_history:
        all_metrics.append(server.recall_history)
        metric_labels.append('Recall')
    if server.f1_history:
        all_metrics.append(server.f1_history)
        metric_labels.append('F1')
    
    if all_metrics:
        ax9.boxplot(all_metrics, labels=metric_labels)
        ax9.set_ylabel('Score')
        ax9.set_title('Metric Distribution Across Rounds')
        ax9.grid(True, alpha=0.3)
    
    plt.suptitle(f'FedAvg Baseline - {scenario_name}', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{save_dir}/fedavg_comprehensive_{scenario_name.lower().replace(" ", "_")}.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # Create individual metric plots
    fig2, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()
    
    metrics_to_plot = [
        ('Accuracy', server.accuracy_history, 'blue'),
        ('Precision', server.precision_history, 'green'),
        ('Recall', server.recall_history, 'red'),
        ('F1-Score', server.f1_history, 'purple')
    ]
    
    for idx, (metric_name, history, color) in enumerate(metrics_to_plot):
        if history:
            axes[idx].plot(range(1, len(history) + 1), history, color=color, linewidth=2)
            axes[idx].set_xlabel('Round')
            axes[idx].set_ylabel(metric_name)
            axes[idx].set_title(f'{metric_name} Over Rounds')
            axes[idx].grid(True, alpha=0.3)
            axes[idx].set_ylim([0, 1])
    
    plt.suptitle(f'FedAvg Individual Metrics - {scenario_name}', fontsize=14)
    plt.tight_layout()
    plt.savefig(f'{save_dir}/fedavg_individual_metrics_{scenario_name.lower().replace(" ", "_")}.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"\nPlots saved to {save_dir}/")
    print(f"  - fedavg_comprehensive_{scenario_name.lower().replace(' ', '_')}.png")
    print(f"  - fedavg_individual_metrics_{scenario_name.lower().replace(' ', '_')}.png")

def print_detailed_performance(metrics, scenario_name):
    """Print detailed performance metrics"""
    print("\n" + "="*70)
    print(f"FEDAVG BASELINE - DETAILED PERFORMANCE ({scenario_name})")
    print("="*70)
    
    print("\nCLASSIFICATION METRICS:")
    print("-" * 50)
    print(f"{'Accuracy:':<15} {metrics.get('accuracy', 0)*100:>6.2f}%")
    print(f"{'Precision:':<15} {metrics.get('precision', 0)*100:>6.2f}%")
    print(f"{'Recall:':<15} {metrics.get('recall', 0)*100:>6.2f}%")
    print(f"{'F1-Score:':<15} {metrics.get('f1_score', 0)*100:>6.2f}%")
    #print(f"{'AUC-ROC:':<15} {metrics.get('auc_roc', 0)*100:>6.2f}%")
    
    print("\nSTATISTICAL SUMMARY:")
    print("-" * 50)
    print(f"{'Final Accuracy:':<20} {metrics.get('final_accuracy', 0)*100:>6.2f}%")
    print(f"{'Max Accuracy:':<20} {metrics.get('max_accuracy', 0)*100:>6.2f}%")
    print(f"{'Mean Accuracy:':<20} {metrics.get('mean_accuracy', 0)*100:>6.2f}%")
    print(f"{'Std Accuracy:':<20} {metrics.get('std_accuracy', 0)*100:>6.2f}%")
    print(f"{'Avg Loss:':<20} {metrics.get('avg_loss', 0):>6.4f}")
    
    print("\nPARTICIPANT COMPOSITION:")
    print("-" * 50)
    print(f"{'Active Honest:':<20} {metrics.get('active_honest', 0)}")
    print(f"{'Active Adversarial:':<20} {metrics.get('active_adversarial', 0)}")
    print(f"{'Total Active:':<20} {metrics.get('total_active', 0)}")
    print(f"{'Adversarial Ratio:':<20} {metrics.get('adversarial_ratio', 0)*100:>6.1f}%")
    
    print("\nCONFUSION MATRIX (Final Round):")
    print("-" * 50)
    if metrics.get('confusion_matrix'):
        cm = np.array(metrics['confusion_matrix'])
        print(f"                Predicted")
        print(f"                Honest   Adversarial")
        print(f"Actual Honest    {cm[0,0]:^8} {cm[0,1]:^11}")
        print(f"Actual Adversarial {cm[1,0]:^8} {cm[1,1]:^11}")
    
    print("\nPERFORMANCE INTERPRETATION:")
    print("-" * 50)
    if metrics.get('accuracy', 0) > 0.85:
        print("✓ Excellent model performance")
    elif metrics.get('accuracy', 0) > 0.75:
        print("✓ Good model performance")
    elif metrics.get('accuracy', 0) > 0.65:
        print("⚠️ Fair model performance - adversarial impact detected")
    else:
        print("✗ Poor model performance - significant adversarial impact")
    
    if metrics.get('f1_score', 0) > 0.8:
        print("✓ Good balance between precision and recall")
    elif metrics.get('f1_score', 0) > 0.6:
        print("⚠️ Moderate balance - room for improvement")
    else:
        print("✗ Poor balance - check precision/recall tradeoff")
    
    if metrics.get('adversarial_ratio', 0) > 0.5:
        print("⚠️ Adversarial majority - FedAvg is vulnerable")
    else:
        print("✓ Honest majority - FedAvg can perform well")
    
    print("\n" + "="*70)

# ===== MAIN FEDAVG SIMULATION =====
def run_fedavg_simulation(dataset_path, num_honest, num_adversarial, rounds=35, device='cpu', save_dir='.'):
    """Main FedAvg simulation function"""
    set_full_determinism()
    
    scenario_name = "Honest Majority" if num_honest > num_adversarial else "Adversarial Majority"
    
    print(f"\n{'='*70}")
    print(f"FEDAVG BASELINE - {scenario_name}")
    print(f"{'='*70}")
    
    print(f"\nLoading dataset from {dataset_path}...")
    dataset = V2GDataset(dataset_path)

    global_model = V2GClassifier(input_size=dataset.num_features, num_classes=dataset.num_classes).to(device)
    server = FedAvgServer(global_model, num_honest, num_adversarial, device, dataset)

    indices = list(range(len(dataset)))
    rng = random.Random(42)
    rng.shuffle(indices)
    train_size = int(0.8 * len(indices))
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    val_dataset = Subset(dataset, val_indices)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, 
                           generator=data_loader_generator, worker_init_fn=seed_worker_fn)

    print("\nDistributing data to participants...")
    activated_honest, activated_adversarial = distribute_data_to_participants_fedavg(server, dataset, train_indices)

    print(f"\nStarting FedAvg with {activated_honest} honest, {activated_adversarial} adversarial")
    print(f"Total rounds: {rounds}")
    print("Note: FedAvg uses standard averaging with NO adversarial defenses")
    
    # Initialize all participant models with global model
    for pid in server.active_participants:
        server.participants[pid]['model'].load_state_dict(global_model.state_dict())
    
    # Training loop
    for round_idx in range(1, rounds + 1):
        server.current_round = round_idx
        print(f"\n{'='*40}")
        print(f"FedAvg Round {round_idx}/{rounds}")
        print(f"{'='*40}")
        
        # Select participants
        selected_pids = server.select_participants(fraction=0.5)
        print(f"Selected {len(selected_pids)} participants")
        
        # Train selected participants
        for pid in selected_pids:
            if not server.participants[pid]['active']:
                continue
                
            is_adversarial = server.ground_truth_types[pid] == 'adversarial'
            
            loss, accuracy = train_participant_fedavg(
                server.participants[pid], 
                dataset, 
                device, 
                round_idx, 
                pid, 
                is_adversarial
            )
            
            if is_adversarial:
                print(f"  Participant {pid} (Adversarial): Loss={loss:.4f}, Local Acc={accuracy:.3f}")
            else:
                print(f"  Participant {pid} (Honest): Loss={loss:.4f}, Local Acc={accuracy:.3f}")
        
        # Federated averaging
        if selected_pids:
            server.federated_averaging(selected_pids)
            
            # Update all participant models
            for pid in server.active_participants:
                server.participants[pid]['model'].load_state_dict(global_model.state_dict())
        
        # Evaluate global model
        accuracy, precision, recall, f1, auc_roc, avg_loss, cm = server.evaluate_model(val_loader)
        
        # Store metrics
        server.accuracy_history.append(accuracy)
        server.precision_history.append(precision)
        server.recall_history.append(recall)
        server.f1_history.append(f1)
        server.loss_history.append(avg_loss)
        server.confusion_matrices.append(cm)
        
        metrics = server.get_metrics()
        print(f"\nGlobal Model Metrics:")
        print(f"  Accuracy:  {accuracy:.3f}")
        print(f"  Precision: {precision:.3f}")
        print(f"  Recall:    {recall:.3f}")
        print(f"  F1-Score:  {f1:.3f}")
        #print(f"  AUC-ROC:   {auc_roc:.3f}")
        print(f"  Loss:      {avg_loss:.4f}")
        print(f"\nActive Participants: {metrics['total_active']} (H: {metrics['active_honest']}, A: {metrics['active_adversarial']})")
        
        # Early stopping check
        if len(server.accuracy_history) >= 10:
            recent_acc = server.accuracy_history[-10:]
            if max(recent_acc) - min(recent_acc) < 0.005:
                print(f"\nEarly stopping: Accuracy plateaued at round {round_idx}")
                break
    
    print(f"\n{'='*70}")
    print("FedAvg Training Complete")
    print(f"{'='*70}")
    
    # Calculate final comprehensive metrics (now JSON serializable)
    final_metrics = calculate_comprehensive_metrics(server, val_loader)
    
    # Create visualizations
    create_comprehensive_plots(server, final_metrics, scenario_name, save_dir)
    
    # Print detailed performance
    print_detailed_performance(final_metrics, scenario_name)
    
    # Save metrics to JSON
    metrics_file = f'{save_dir}/fedavg_metrics_{scenario_name.lower().replace(" ", "_")}.json'
    with open(metrics_file, 'w') as f:
        # Use the already serializable metrics dictionary
        json.dump(final_metrics, f, indent=4, default=str)
    print(f"\nMetrics saved to: {metrics_file}")
    
    return server, final_metrics

def compare_scenarios(fedavg_metrics_honest, fedavg_metrics_adv):
    """Compare performance between honest and adversarial majority scenarios"""
    
    print("\n" + "="*80)
    print("SCENARIO COMPARISON: Honest Majority vs Adversarial Majority")
    print("="*80)
    
    print(f"\n{'Metric':<25} {'Honest Majority':>20} {'Adversarial Majority':>20}")
    print("-" * 70)
    
    # Classification metrics
    metrics_to_compare = [
        ('Accuracy', 'accuracy'),
        ('Precision', 'precision'),
        ('Recall', 'recall'),
        ('F1-Score', 'f1_score'),
        #('AUC-ROC', 'auc_roc')
    ]
    
    for display_name, metric_key in metrics_to_compare:
        hon_val = fedavg_metrics_honest.get(metric_key, 0) * 100
        adv_val = fedavg_metrics_adv.get(metric_key, 0) * 100
        diff = adv_val - hon_val
        diff_sign = '+' if diff >= 0 else ''
        
        print(f"{display_name:<25} {hon_val:>18.2f}% {adv_val:>18.2f}% ({diff_sign}{diff:+.2f}%)")
    
    print("-" * 70)
    
    # Statistical metrics
    print(f"\n{'Statistical Metric':<25} {'Honest Majority':>20} {'Adversarial Majority':>20}")
    print("-" * 70)
    
    stat_metrics = [
        ('Max Accuracy', 'max_accuracy'),
        ('Mean Accuracy', 'mean_accuracy'),
        ('Std Accuracy', 'std_accuracy')
    ]
    
    for display_name, metric_key in stat_metrics:
        hon_val = fedavg_metrics_honest.get(metric_key, 0) * 100
        adv_val = fedavg_metrics_adv.get(metric_key, 0) * 100
        
        print(f"{display_name:<25} {hon_val:>18.2f}% {adv_val:>18.2f}%")
    
    print("-" * 70)
    
    # Participant metrics
    print(f"\n{'Participant Metric':<25} {'Honest Majority':>20} {'Adversarial Majority':>20}")
    print("-" * 70)
    
    part_metrics = [
        ('Active Honest', 'active_honest'),
        ('Active Adversarial', 'active_adversarial'),
        ('Adversarial Ratio', 'adversarial_ratio')
    ]
    
    for display_name, metric_key in part_metrics:
        hon_val = fedavg_metrics_honest.get(metric_key, 0)
        adv_val = fedavg_metrics_adv.get(metric_key, 0)
        
        if metric_key == 'adversarial_ratio':
            print(f"{display_name:<25} {hon_val*100:>18.1f}% {adv_val*100:>18.1f}%")
        else:
            print(f"{display_name:<25} {int(hon_val):>18} {int(adv_val):>18}")
    
    print("-" * 70)
    
    # Performance degradation analysis
    print(f"\nPERFORMANCE DEGRADATION ANALYSIS:")
    print("-" * 70)
    
    if fedavg_metrics_honest.get('accuracy', 0) > 0:
        accuracy_degradation = (fedavg_metrics_honest['accuracy'] - fedavg_metrics_adv['accuracy']) / fedavg_metrics_honest['accuracy'] * 100
    else:
        accuracy_degradation = 0
    
    if fedavg_metrics_honest.get('f1_score', 0) > 0:
        f1_degradation = (fedavg_metrics_honest['f1_score'] - fedavg_metrics_adv['f1_score']) / fedavg_metrics_honest['f1_score'] * 100
    else:
        f1_degradation = 0
    
    print(f"Accuracy Degradation: {accuracy_degradation:.1f}%")
    print(f"F1-Score Degradation: {f1_degradation:.1f}%")
    
    if accuracy_degradation > 10:
        print("⚠️ Significant performance degradation in adversarial majority scenario")
        print("   This shows FedAvg's vulnerability to adversarial participants")
    elif accuracy_degradation > 5:
        print("⚠️ Moderate performance degradation in adversarial majority scenario")
    else:
        print("✓ Minimal performance degradation - FedAvg shows robustness")
    
    print("\n" + "="*80)

# ===== MAIN EXECUTION =====
if __name__ == "__main__":
    device = 'cpu'
    dataset_path = r"C:\Users\Administrator\Desktop\v2g dataset NREL.csv"
    
    # Create directories
    os.makedirs('./fedavg_honest_majority', exist_ok=True)
    os.makedirs('./fedavg_adversarial_majority', exist_ok=True)
    os.makedirs('./comparison_results', exist_ok=True)
    
    print("\n" + "="*80)
    print("FEDAVG BASELINE CLASSIFIER WITH COMPLETE METRICS")
    print("="*80)
    
    # Run Honest Majority Scenario
    print("\n" + "="*80)
    print("RUNNING HONEST MAJORITY SCENARIO (10 Honest, 9 Adversarial)")
    print("="*80)
    
    server_honest, metrics_honest = run_fedavg_simulation(
        dataset_path, num_honest=10, num_adversarial=9, rounds=10, device=device,
        save_dir='./fedavg_honest_majority'
    )
    
    # Run Adversarial Majority Scenario
    print("\n" + "="*80)
    print("RUNNING ADVERSARIAL MAJORITY SCENARIO (10 Honest, 11 Adversarial)")
    print("="*80)
    
    server_adv, metrics_adv = run_fedavg_simulation(
        dataset_path, num_honest=10, num_adversarial=11, rounds=15, device=device,
        save_dir='./fedavg_adversarial_majority'
    )
    
    # Compare scenarios
    compare_scenarios(metrics_honest, metrics_adv)
    
    # Generate summary report
    print("\n" + "="*80)
    print("FINAL SUMMARY REPORT")
    print("="*80)
    
    print(f"\nFedAvg Baseline Evaluation Complete:")
    print(f"  • Honest Majority: Accuracy = {metrics_honest.get('accuracy', 0)*100:.1f}%, F1 = {metrics_honest.get('f1_score', 0)*100:.1f}%")
    print(f"  • Adversarial Majority: Accuracy = {metrics_adv.get('accuracy', 0)*100:.1f}%, F1 = {metrics_adv.get('f1_score', 0)*100:.1f}%")
    
    # Calculate overall performance
    avg_accuracy = (metrics_honest.get('accuracy', 0) + metrics_adv.get('accuracy', 0)) / 2 * 100
    avg_f1 = (metrics_honest.get('f1_score', 0) + metrics_adv.get('f1_score', 0)) / 2 * 100
    
    print(f"\nAverage Performance Across Scenarios:")
    print(f"  • Average Accuracy: {avg_accuracy:.1f}%")
    print(f"  • Average F1-Score: {avg_f1:.1f}%")
    
    print(f"\nResults saved in:")
    print(f"  • ./fedavg_honest_majority/")
    print(f"  • ./fedavg_adversarial_majority/")
    print(f"  • ./comparison_results/")
    
    print("\n" + "="*80)