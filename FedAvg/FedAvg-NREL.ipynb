{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IMPROVED FEDAVG WITH ALL ENHANCEMENTS\n",
      "================================================================================\n",
      "\n",
      "Improvements:\n",
      "  ✓ Reduced model capacity (256 vs 512 hidden)\n",
      "  ✓ Class-weighted CrossEntropyLoss\n",
      "  ✓ Minimal data augmentation (0.02 noise)\n",
      "  ✓ Less aggressive SMOTE (0.8 ratio)\n",
      "  ✓ Reduced adversarial poisoning (20% → 10%)\n",
      "  ✓ Optimal threshold tuning\n",
      "  ✓ Stratified train-val split\n",
      "  ✓ BatchNorm + LeakyReLU\n",
      "  ✓ AdamW with weight decay\n",
      "  ✓ Prediction diagnostics\n",
      "\n",
      "================================================================================\n",
      "SCENARIO 1: HONEST MAJORITY (10 Honest, 9 Adversarial)\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "IMPROVED FEDAVG - Honest Majority\n",
      "======================================================================\n",
      "\n",
      "Loading dataset from C:\\Users\\Administrator\\Desktop\\v2g dataset kaggle.csv...\n",
      "Dataset loaded: 1000 records, columns: ['participant_id', 'timestamp', 'battery_capacity_kWh', 'current_charge_kWh', 'discharge_rate_kW', 'energy_requested_kWh', 'label']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_full_determinism():\n",
    "    \"\"\"Set all random seeds for complete reproducibility\"\"\"\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(42)\n",
    "    return g, seed_worker\n",
    "\n",
    "data_loader_generator, seed_worker_fn = set_full_determinism()\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# ===== IMPROVED DATASET CLASS =====\n",
    "class V2GDataset(Dataset):\n",
    "    def __init__(self, csv_path, use_augmentation=True, use_smote=True):\n",
    "        dataset_rng = random.Random(42)\n",
    "        np_rng = np.random.RandomState(42)\n",
    "        \n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        print(f\"Dataset loaded: {len(self.data)} records, columns: {list(self.data.columns)}\")\n",
    "\n",
    "        # Temporal features\n",
    "        if 'timestamp' in self.data.columns:\n",
    "            self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])\n",
    "            self.data['hour'] = self.data['timestamp'].dt.hour\n",
    "            self.data['day_of_week'] = self.data['timestamp'].dt.dayofweek\n",
    "            self.data['is_weekend'] = (self.data['day_of_week'] >= 5).astype(int)\n",
    "        else:\n",
    "            self.data['hour'] = 0\n",
    "            self.data['day_of_week'] = 0\n",
    "            self.data['is_weekend'] = 0\n",
    "\n",
    "        # Engineered features\n",
    "        eps = 1e-6\n",
    "        self.data['charge_discharge_ratio'] = self.data['current_charge_kWh'] / (self.data['discharge_rate_kW'] + eps)\n",
    "        self.data['energy_hour_interaction'] = self.data['energy_requested_kWh'] * self.data['hour']\n",
    "        self.data['charge_capacity_ratio'] = self.data['current_charge_kWh'] / (self.data['battery_capacity_kWh'] + eps)\n",
    "        self.data['efficiency_estimate'] = self.data['energy_requested_kWh'] / (self.data['discharge_rate_kW'] * 0.25 + eps)\n",
    "\n",
    "        # Validate labels\n",
    "        if 'label' not in self.data.columns:\n",
    "            raise ValueError(\"Dataset must contain 'label' column\")\n",
    "        \n",
    "        self.data['label'] = self.data['label'].astype(str).str.lower().str.strip()\n",
    "        valid_labels = ['honest', 'adversarial']\n",
    "        self.data = self.data[self.data['label'].isin(valid_labels)]\n",
    "        \n",
    "        if self.data.empty:\n",
    "            raise ValueError(\"No valid labels found. Expected 'honest' or 'adversarial'\")\n",
    "\n",
    "        self.data = self.data.dropna(subset=['label'])\n",
    "\n",
    "        if 'participant_id' not in self.data.columns:\n",
    "            raise ValueError(\"Dataset must contain 'participant_id' column\")\n",
    "\n",
    "        # Ground truth participant labels\n",
    "        self.participant_ground_truth = {}\n",
    "        for pid in self.data['participant_id'].unique():\n",
    "            pid_data = self.data[self.data['participant_id'] == pid]\n",
    "            label_counts = pid_data['label'].value_counts()\n",
    "            majority_label = label_counts.idxmax()\n",
    "            self.participant_ground_truth[int(pid)] = majority_label\n",
    "        \n",
    "        print(\"\\n=== GROUND TRUTH PARTICIPANT LABELS ===\")\n",
    "        honest_pids = [pid for pid, label in self.participant_ground_truth.items() if label == 'honest']\n",
    "        adv_pids = [pid for pid, label in self.participant_ground_truth.items() if label == 'adversarial']\n",
    "        print(f\"Honest: {sorted(honest_pids)} (n={len(honest_pids)})\")\n",
    "        print(f\"Adversarial: {sorted(adv_pids)} (n={len(adv_pids)})\")\n",
    "\n",
    "        # Feature columns\n",
    "        feature_cols = [\n",
    "            'battery_capacity_kWh', 'current_charge_kWh', 'discharge_rate_kW',\n",
    "            'energy_requested_kWh', 'hour', 'charge_discharge_ratio',\n",
    "            'energy_hour_interaction', 'charge_capacity_ratio',\n",
    "            'efficiency_estimate', 'day_of_week', 'is_weekend'\n",
    "        ]\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            if col not in self.data.columns:\n",
    "                raise ValueError(f\"Missing column: {col}\")\n",
    "\n",
    "        self.features = self.data[feature_cols].values\n",
    "        self.labels = self.data['label'].values\n",
    "        self.participant_ids = self.data['participant_id'].values\n",
    "        self.original_indices = np.arange(len(self.data))\n",
    "\n",
    "        # Label encoding\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "        \n",
    "        # Print original class distribution\n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        print(f\"\\nOriginal class distribution: {dict(zip(self.label_encoder.classes_, counts))}\")\n",
    "\n",
    "        # IMPROVED: Minimal data augmentation\n",
    "        if use_augmentation:\n",
    "            print(\"Applying minimal data augmentation...\")\n",
    "            np_rng = np.random.RandomState(42)\n",
    "            X_aug = self.features.copy()\n",
    "            \n",
    "            # Only small Gaussian noise\n",
    "            noise = np_rng.normal(0, 0.02, size=X_aug.shape)  # Reduced from 0.1\n",
    "            X_aug = X_aug + noise\n",
    "            \n",
    "            # Minimal scaling only\n",
    "            scale_factors = np_rng.uniform(0.98, 1.02, size=(X_aug.shape[0], 1))\n",
    "            X_aug = X_aug * scale_factors\n",
    "            \n",
    "            y_aug = self.labels\n",
    "            self.features = np.vstack([self.features, X_aug])\n",
    "            self.labels = np.concatenate([self.labels, y_aug])\n",
    "            self.participant_ids = np.concatenate([self.participant_ids, self.participant_ids])\n",
    "            self.original_indices = np.concatenate([self.original_indices, self.original_indices])\n",
    "\n",
    "        # IMPROVED: Less aggressive SMOTE\n",
    "        if use_smote:\n",
    "            print(\"Applying SMOTE with strategy=0.8...\")\n",
    "            unique_before, counts_before = np.unique(self.labels, return_counts=True)\n",
    "            print(f\"Before SMOTE: {dict(zip(unique_before, counts_before))}\")\n",
    "            \n",
    "            smote = SMOTE(sampling_strategy=0.8, k_neighbors=5, random_state=42)\n",
    "            self.features, self.labels = smote.fit_resample(self.features, self.labels)\n",
    "            \n",
    "            unique_after, counts_after = np.unique(self.labels, return_counts=True)\n",
    "            print(f\"After SMOTE: {dict(zip(unique_after, counts_after))}\")\n",
    "\n",
    "        # Ensure minimum samples per participant\n",
    "        for pid in np.unique(self.participant_ids):\n",
    "            idx = np.where(self.participant_ids == pid)[0]\n",
    "            if len(idx) < 10:\n",
    "                num_needed = 10 - len(idx)\n",
    "                participant_rng = np.random.RandomState(42 + int(pid))\n",
    "                indices_to_duplicate = participant_rng.choice(idx, size=num_needed, replace=True)\n",
    "                extra_samples = self.features[indices_to_duplicate]\n",
    "                extra_labels = self.labels[indices_to_duplicate]\n",
    "\n",
    "                self.features = np.vstack([self.features, extra_samples])\n",
    "                self.labels = np.concatenate([self.labels, extra_labels])\n",
    "                self.participant_ids = np.concatenate([self.participant_ids, [pid] * len(extra_samples)])\n",
    "                self.original_indices = np.concatenate([self.original_indices, [self.original_indices[idx[0]]] * len(extra_samples)])\n",
    "\n",
    "        # Standardization\n",
    "        self.scaler = StandardScaler()\n",
    "        self.features = self.scaler.fit_transform(self.features)\n",
    "\n",
    "        self.num_features = self.features.shape[1]\n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.features = torch.tensor(self.features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "        # Participant indices mapping\n",
    "        self.participant_indices = defaultdict(list)\n",
    "        for idx, pid in enumerate(self.participant_ids):\n",
    "            self.participant_indices[pid].append(idx)\n",
    "        \n",
    "        # Final class distribution\n",
    "        unique_final, counts_final = np.unique(self.labels.numpy(), return_counts=True)\n",
    "        print(f\"\\nFinal class distribution: {dict(zip(self.label_encoder.classes_, counts_final))}\")\n",
    "        print(f\"Class balance ratio: {counts_final[1]/counts_final[0]:.2f}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "    def get_participant_ground_truth(self, participant_id):\n",
    "        return self.participant_ground_truth.get(int(participant_id), None)\n",
    "\n",
    "    def get_participant_indices(self, participant_id):\n",
    "        return self.participant_indices.get(participant_id, [])\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        \"\"\"Calculate class weights for loss function\"\"\"\n",
    "        counts = Counter(self.labels.numpy())\n",
    "        total = sum(counts.values())\n",
    "        weights = [total / (len(counts) * counts[i]) for i in range(len(counts))]\n",
    "        return torch.FloatTensor(weights)\n",
    "\n",
    "# ===== IMPROVED MODEL ARCHITECTURE =====\n",
    "class V2GClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256, num_classes=2, dropout=0.3):\n",
    "        super(V2GClassifier, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Reduced capacity to prevent overfitting\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)  # LayerNorm works with batch_size=1\n",
    "        self.relu = nn.LeakyReLU(0.2)  # Better gradients\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.hidden_layer1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size // 2)\n",
    "        \n",
    "        self.hidden_layer2 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.norm3 = nn.LayerNorm(hidden_size // 4)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size // 4, num_classes)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.norm1(self.input_layer(x))))\n",
    "        x = self.dropout(self.relu(self.norm2(self.hidden_layer1(x))))\n",
    "        x = self.dropout(self.relu(self.norm3(self.hidden_layer2(x))))\n",
    "        x = self.output_layer(x)\n",
    "        return x  # No softmax - CrossEntropyLoss handles it\n",
    "\n",
    "# ===== IMPROVED FEDAVG SERVER =====\n",
    "class FedAvgServer:\n",
    "    def __init__(self, model, num_honest, num_adversarial, device='cpu', dataset=None, class_weights=None):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.num_honest = num_honest\n",
    "        self.num_adversarial = num_adversarial\n",
    "        self.dataset = dataset\n",
    "        self.current_round = 0\n",
    "        self.participants = {}\n",
    "        self.ground_truth_types = {}\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.accuracy_history = []\n",
    "        self.precision_history = []\n",
    "        self.recall_history = []\n",
    "        self.f1_history = []\n",
    "        self.loss_history = []\n",
    "        self.confusion_matrices = []\n",
    "        \n",
    "        self.active_participants = []\n",
    "        self.honest_removed_count = 0\n",
    "        self.adversarial_removed_count = 0\n",
    "        \n",
    "        # Threshold optimization\n",
    "        self.optimal_threshold = 0.5\n",
    "        \n",
    "        self._initialize_participants(dataset)\n",
    "    \n",
    "    def _initialize_participants(self, dataset):\n",
    "        available_pids = list(set(dataset.participant_ids))\n",
    "        \n",
    "        honest_ground_truth = []\n",
    "        adversarial_ground_truth = []\n",
    "        \n",
    "        for pid in available_pids:\n",
    "            ground_truth = dataset.get_participant_ground_truth(pid)\n",
    "            if ground_truth == 'honest':\n",
    "                honest_ground_truth.append(pid)\n",
    "            elif ground_truth == 'adversarial':\n",
    "                adversarial_ground_truth.append(pid)\n",
    "        \n",
    "        print(f\"\\nFedAvg - Dataset composition:\")\n",
    "        print(f\"  {len(honest_ground_truth)} honest: {sorted(honest_ground_truth)}\")\n",
    "        print(f\"  {len(adversarial_ground_truth)} adversarial: {sorted(adversarial_ground_truth)}\")\n",
    "        \n",
    "        if len(honest_ground_truth) < self.num_honest:\n",
    "            print(f\"Warning: Only {len(honest_ground_truth)} honest available, adjusting from {self.num_honest}\")\n",
    "            self.num_honest = len(honest_ground_truth)\n",
    "        \n",
    "        if len(adversarial_ground_truth) < self.num_adversarial:\n",
    "            print(f\"Warning: Only {len(adversarial_ground_truth)} adversarial available, adjusting from {self.num_adversarial}\")\n",
    "            self.num_adversarial = len(adversarial_ground_truth)\n",
    "        \n",
    "        rng = random.Random(42)\n",
    "        honest_sorted = sorted(honest_ground_truth)\n",
    "        adversarial_sorted = sorted(adversarial_ground_truth)\n",
    "        rng.shuffle(honest_sorted)\n",
    "        rng.shuffle(adversarial_sorted)\n",
    "        \n",
    "        honest_ids = honest_sorted[:self.num_honest]\n",
    "        adversarial_ids = adversarial_sorted[:self.num_adversarial]\n",
    "\n",
    "        def _new_participant(pid):\n",
    "            torch.manual_seed(42 + pid)\n",
    "            model = V2GClassifier(self.model.input_layer.in_features, \n",
    "                                num_classes=self.model.output_layer.out_features).to(self.device)\n",
    "            \n",
    "            return {\n",
    "                'model': model,\n",
    "                'type': None,\n",
    "                'data': None,\n",
    "                'active': True,\n",
    "                'data_size': 0,\n",
    "                'train_loss': 0.0,\n",
    "                'local_accuracy': 0.0\n",
    "            }\n",
    "\n",
    "        for idx in honest_ids:\n",
    "            p = _new_participant(idx)\n",
    "            p['type'] = 'honest'\n",
    "            self.participants[idx] = p\n",
    "            self.ground_truth_types[idx] = dataset.get_participant_ground_truth(idx)\n",
    "\n",
    "        for idx in adversarial_ids:\n",
    "            p = _new_participant(idx)\n",
    "            p['type'] = 'adversarial'\n",
    "            self.participants[idx] = p\n",
    "            self.ground_truth_types[idx] = dataset.get_participant_ground_truth(idx)\n",
    "\n",
    "        print(f\"\\nFedAvg Server initialized: {self.num_honest} honest, {self.num_adversarial} adversarial\")\n",
    "        self.active_participants = list(self.participants.keys())\n",
    "    \n",
    "    def select_participants(self, fraction=0.5):\n",
    "        rng = random.Random(42 + self.current_round)\n",
    "        num_to_select = max(1, int(len(self.active_participants) * fraction))\n",
    "        selected = rng.sample(self.active_participants, num_to_select)\n",
    "        return selected\n",
    "    \n",
    "    def federated_averaging(self, participant_updates):\n",
    "        if not participant_updates:\n",
    "            return\n",
    "        \n",
    "        total_data_size = sum(self.participants[pid]['data_size'] for pid in participant_updates)\n",
    "        \n",
    "        averaged_params = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for pid in participant_updates:\n",
    "                participant_model = self.participants[pid]['model']\n",
    "                weight = self.participants[pid]['data_size'] / total_data_size\n",
    "                \n",
    "                for name, param in participant_model.named_parameters():\n",
    "                    if pid == participant_updates[0]:\n",
    "                        averaged_params[name] = param.data.clone() * weight\n",
    "                    else:\n",
    "                        averaged_params[name] += param.data.clone() * weight\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.data.copy_(averaged_params[name])\n",
    "    \n",
    "    def evaluate_model(self, val_loader, return_predictions=False, use_optimal_threshold=False):\n",
    "        \"\"\"Evaluate model with full metrics and threshold tuning\"\"\"\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(weight=self.class_weights.to(self.device) if self.class_weights is not None else None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Apply threshold\n",
    "        if use_optimal_threshold:\n",
    "            threshold = self.optimal_threshold\n",
    "        else:\n",
    "            threshold = 0.5\n",
    "        \n",
    "        all_preds = [1 if p[1] > threshold else 0 for p in all_probs]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        \n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        if len(np.unique(all_labels)) > 1:\n",
    "            try:\n",
    "                auc_roc = roc_auc_score(all_labels, [p[1] for p in all_probs])\n",
    "            except:\n",
    "                auc_roc = 0.0\n",
    "        else:\n",
    "            auc_roc = 0.0\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        \n",
    "        if return_predictions:\n",
    "            return accuracy, precision, recall, f1, auc_roc, avg_loss, cm, all_preds, all_labels, all_probs\n",
    "        else:\n",
    "            return accuracy, precision, recall, f1, auc_roc, avg_loss, cm\n",
    "    \n",
    "    def optimize_threshold(self, val_loader):\n",
    "        \"\"\"Find optimal decision threshold using F1 score\"\"\"\n",
    "        _, _, _, _, _, _, _, _, all_labels, all_probs = self.evaluate_model(val_loader, return_predictions=True)\n",
    "        \n",
    "        probs_positive = [p[1] for p in all_probs]\n",
    "        \n",
    "        if len(np.unique(all_labels)) > 1:\n",
    "            precisions, recalls, thresholds = precision_recall_curve(all_labels, probs_positive)\n",
    "            f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "            best_idx = np.argmax(f1_scores)\n",
    "            self.optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "            print(f\"  Optimal threshold: {self.optimal_threshold:.3f} (F1: {f1_scores[best_idx]:.3f})\")\n",
    "        else:\n",
    "            self.optimal_threshold = 0.5\n",
    "    \n",
    "    def diagnose_predictions(self, val_loader):\n",
    "        \"\"\"Diagnose model prediction distribution\"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in val_loader:\n",
    "                outputs = self.model(inputs.to(self.device))\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "        \n",
    "        unique, counts = np.unique(predictions, return_counts=True)\n",
    "        total = sum(counts)\n",
    "        print(f\"\\n  Prediction distribution: {dict(zip(unique, counts))}\")\n",
    "        for cls, count in zip(unique, counts):\n",
    "            print(f\"    Class {cls}: {count/total*100:.1f}%\")\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        active_honest = sum(1 for idx in self.active_participants \n",
    "                       if self.ground_truth_types[idx] == 'honest')\n",
    "        active_adversarial = sum(1 for idx in self.active_participants \n",
    "                            if self.ground_truth_types[idx] == 'adversarial')\n",
    "        total_active = len(self.active_participants)\n",
    "        adversarial_ratio = active_adversarial / total_active if total_active > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'active_honest': active_honest,\n",
    "            'active_adversarial': active_adversarial,\n",
    "            'total_active': total_active,\n",
    "            'adversarial_ratio': adversarial_ratio,\n",
    "            'honest_removed_count': self.honest_removed_count,\n",
    "            'adversarial_removed_count': self.adversarial_removed_count,\n",
    "            'current_round': self.current_round\n",
    "        }\n",
    "\n",
    "# ===== IMPROVED TRAINING FUNCTION =====\n",
    "def train_participant_fedavg(participant, dataset, device, current_round, participant_id, \n",
    "                            is_adversarial=False, class_weights=None):\n",
    "    model = participant['model']\n",
    "    model.train()\n",
    "    \n",
    "    # Better learning rate schedule\n",
    "    lr = 0.001 * (0.9 ** (current_round // 5))\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Use CrossEntropyLoss with class weights\n",
    "    if class_weights is not None:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    data_loader = participant['data']\n",
    "    if not data_loader:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # REDUCED adversarial poisoning strength\n",
    "    adv_rng = random.Random(42 + participant_id + current_round * 1000)\n",
    "    noise_scale = 0.2 if current_round < 5 else 0.1  # Reduced from 0.6/0.4\n",
    "    flip_prob = 0.2 if current_round < 5 else 0.1    # Reduced from 0.4/0.3\n",
    "    \n",
    "    training_epochs = 5  # Increased from 3\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for features, labels in data_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            if is_adversarial:\n",
    "                # Reduced poisoning\n",
    "                if adv_rng.random() < flip_prob:\n",
    "                    labels = (labels + 1) % dataset.num_classes\n",
    "                if adv_rng.random() < 0.5:\n",
    "                    torch.manual_seed(42 + participant_id + current_round * 1000 + epoch)\n",
    "                    noise = torch.randn_like(features) * noise_scale\n",
    "                    features = features + noise\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        total_loss += epoch_loss / len(data_loader)\n",
    "    \n",
    "    avg_loss = total_loss / training_epochs\n",
    "    local_accuracy = correct / total if total > 0 else 0.0\n",
    "    \n",
    "    participant['train_loss'] = avg_loss\n",
    "    participant['local_accuracy'] = local_accuracy\n",
    "    \n",
    "    return avg_loss, local_accuracy\n",
    "\n",
    "# ===== DATA DISTRIBUTION =====\n",
    "def distribute_data_to_participants_fedavg(server, dataset, train_indices):\n",
    "    activated_honest = 0\n",
    "    activated_adversarial = 0\n",
    "\n",
    "    for idx in server.participants:\n",
    "        indices = [i for i in train_indices if i < len(dataset.participant_ids) and dataset.participant_ids[i] == idx]\n",
    "        num_records = len(indices)\n",
    "        \n",
    "        # Require minimum 10 samples to ensure stable training\n",
    "        if num_records < 10:\n",
    "            print(f\"Warning: Participant {idx} has only {num_records} records, deactivating\")\n",
    "            server.participants[idx]['active'] = False\n",
    "            if idx in server.active_participants:\n",
    "                server.active_participants.remove(idx)\n",
    "            continue\n",
    "            \n",
    "        subset = Subset(dataset, indices)\n",
    "        # Ensure batch size is at least 2 and at most 32\n",
    "        batch_size = min(32, max(8, num_records // 3))\n",
    "        \n",
    "        # For very small datasets, use drop_last=True to avoid batch_size=1\n",
    "        drop_last = num_records < 16\n",
    "        \n",
    "        server.participants[idx]['data'] = DataLoader(\n",
    "            subset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            drop_last=drop_last,  # Drop incomplete batches for small datasets\n",
    "            generator=data_loader_generator, \n",
    "            worker_init_fn=seed_worker_fn\n",
    "        )\n",
    "        server.participants[idx]['active'] = True\n",
    "        server.participants[idx]['data_size'] = len(indices)\n",
    "        \n",
    "        if server.ground_truth_types[idx] == 'honest':\n",
    "            activated_honest += 1\n",
    "        else:\n",
    "            activated_adversarial += 1\n",
    "\n",
    "    print(f\"Activated: {activated_honest} honest, {activated_adversarial} adversarial\")\n",
    "    return activated_honest, activated_adversarial\n",
    "\n",
    "# ===== METRICS FUNCTIONS =====\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert numpy arrays and other non-serializable objects to JSON-serializable format\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def calculate_comprehensive_metrics(server, val_loader):\n",
    "    \"\"\"Calculate comprehensive metrics with optimal threshold\"\"\"\n",
    "    \n",
    "    # Optimize threshold\n",
    "    server.optimize_threshold(val_loader)\n",
    "    \n",
    "    # Get final evaluation with optimal threshold\n",
    "    accuracy, precision, recall, f1, auc_roc, avg_loss, cm = server.evaluate_model(\n",
    "        val_loader, use_optimal_threshold=True\n",
    "    )\n",
    "    \n",
    "    metrics = server.get_metrics()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if server.accuracy_history:\n",
    "        final_accuracy = server.accuracy_history[-1]\n",
    "        max_accuracy = max(server.accuracy_history)\n",
    "        mean_accuracy = np.mean(server.accuracy_history)\n",
    "        std_accuracy = np.std(server.accuracy_history)\n",
    "    else:\n",
    "        final_accuracy = max_accuracy = mean_accuracy = std_accuracy = 0.0\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'auc_roc': float(auc_roc),\n",
    "        'avg_loss': float(avg_loss),\n",
    "        'optimal_threshold': float(server.optimal_threshold),\n",
    "        \n",
    "        'final_accuracy': float(final_accuracy),\n",
    "        'max_accuracy': float(max_accuracy),\n",
    "        'mean_accuracy': float(mean_accuracy),\n",
    "        'std_accuracy': float(std_accuracy),\n",
    "        \n",
    "        'accuracy_history': [float(x) for x in server.accuracy_history],\n",
    "        'precision_history': [float(x) for x in server.precision_history],\n",
    "        'recall_history': [float(x) for x in server.recall_history],\n",
    "        'f1_history': [float(x) for x in server.f1_history],\n",
    "        'loss_history': [float(x) for x in server.loss_history],\n",
    "        \n",
    "        'confusion_matrices': [cm.tolist() for cm in server.confusion_matrices] if server.confusion_matrices else [],\n",
    "        \n",
    "        'active_honest': int(metrics['active_honest']),\n",
    "        'active_adversarial': int(metrics['active_adversarial']),\n",
    "        'total_active': int(metrics['total_active']),\n",
    "        'adversarial_ratio': float(metrics['adversarial_ratio']),\n",
    "        'honest_removed_count': int(metrics['honest_removed_count']),\n",
    "        'adversarial_removed_count': int(metrics['adversarial_removed_count']),\n",
    "        'rounds_completed': int(metrics['current_round']),\n",
    "        \n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        \n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'scenario': 'Honest Majority' if metrics['active_honest'] > metrics['active_adversarial'] else 'Adversarial Majority'\n",
    "    }\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "def create_comprehensive_plots(server, metrics, scenario_name, save_dir='.'):\n",
    "    \"\"\"Create comprehensive visualization plots\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Accuracy over rounds\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    if server.accuracy_history:\n",
    "        rounds = range(1, len(server.accuracy_history) + 1)\n",
    "        ax1.plot(rounds, server.accuracy_history, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "        ax1.axhline(y=np.mean(server.accuracy_history), color='r', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(server.accuracy_history):.3f}')\n",
    "        ax1.set_xlabel('Round')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title(f'Accuracy Over Rounds')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "    \n",
    "    # 2. All metrics over rounds\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    if server.accuracy_history:\n",
    "        rounds = range(1, len(server.accuracy_history) + 1)\n",
    "        ax2.plot(rounds, server.accuracy_history, 'b-', label='Accuracy', linewidth=2)\n",
    "        if server.precision_history:\n",
    "            ax2.plot(rounds, server.precision_history, 'g-', label='Precision', linewidth=2)\n",
    "        if server.recall_history:\n",
    "            ax2.plot(rounds, server.recall_history, 'r-', label='Recall', linewidth=2)\n",
    "        if server.f1_history:\n",
    "            ax2.plot(rounds, server.f1_history, 'm-', label='F1', linewidth=2)\n",
    "        ax2.set_xlabel('Round')\n",
    "        ax2.set_ylabel('Score')\n",
    "        ax2.set_title('All Metrics Over Rounds')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        \n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    if metrics.get('confusion_matrix'):\n",
    "        cm = np.array(metrics['confusion_matrix'])\n",
    "        im = ax3.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax3.set_title(f'Confusion Matrix (Final Round)\\nAccuracy: {metrics[\"accuracy\"]:.3f}')\n",
    "        plt.colorbar(im, ax=ax3)\n",
    "        classes = ['Honest', 'Adversarial']\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        ax3.set_xticks(tick_marks)\n",
    "        ax3.set_xticklabels(classes)\n",
    "        ax3.set_yticks(tick_marks)\n",
    "        ax3.set_yticklabels(classes)\n",
    "        \n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax3.text(j, i, format(cm[i, j], 'd'), \n",
    "                        ha=\"center\", va=\"center\", \n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                        fontsize=14)\n",
    "        plt.show() \n",
    "    \n",
    "    # 4. Precision-Recall tradeoff\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    if server.precision_history and server.recall_history:\n",
    "        ax4.scatter(server.recall_history, server.precision_history, alpha=0.6, \n",
    "                   c=range(len(server.precision_history)), cmap='viridis')\n",
    "        ax4.set_xlabel('Recall')\n",
    "        ax4.set_ylabel('Precision')\n",
    "        ax4.set_title('Precision-Recall Tradeoff')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Loss over rounds\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    if server.loss_history:\n",
    "        rounds = range(1, len(server.loss_history) + 1)\n",
    "        ax5.plot(rounds, server.loss_history, 'r-', linewidth=2)\n",
    "        ax5.set_xlabel('Round')\n",
    "        ax5.set_ylabel('Loss')\n",
    "        ax5.set_title('Loss Over Rounds')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Participant composition\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    labels = ['Honest', 'Adversarial']\n",
    "    sizes = [metrics.get('active_honest', 0), metrics.get('active_adversarial', 0)]\n",
    "    colors = ['green', 'red']\n",
    "    ax6.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax6.set_title('Active Participant Composition')\n",
    "    \n",
    "    # 7. Metric comparison bar chart\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    metric_values = [metrics.get('accuracy', 0), metrics.get('precision', 0), \n",
    "                    metrics.get('recall', 0), metrics.get('f1_score', 0)]\n",
    "    colors = ['blue', 'green', 'red', 'purple']\n",
    "    bars = ax7.bar(metric_names, metric_values, color=colors)\n",
    "    ax7.set_ylabel('Score')\n",
    "    ax7.set_title('Final Model Metrics')\n",
    "    ax7.set_ylim([0, 1])\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        height = bar.get_height()\n",
    "        ax7.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 8. Summary text\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    summary_text = f\"\"\"\n",
    "    IMPROVED FedAvg Performance\n",
    "    ===========================\n",
    "    Scenario: {scenario_name}\n",
    "    \n",
    "    Final Metrics:\n",
    "    Accuracy:  {metrics.get('accuracy', 0)*100:.2f}%\n",
    "    Precision: {metrics.get('precision', 0)*100:.2f}%\n",
    "    Recall:    {metrics.get('recall', 0)*100:.2f}%\n",
    "    F1-Score:  {metrics.get('f1_score', 0)*100:.2f}%\n",
    "    AUC-ROC:   {metrics.get('auc_roc', 0)*100:.2f}%\n",
    "    \n",
    "    Threshold: {metrics.get('optimal_threshold', 0.5):.3f}\n",
    "    \n",
    "    Statistics:\n",
    "    Max Accuracy: {metrics.get('max_accuracy', 0)*100:.2f}%\n",
    "    Mean Accuracy: {metrics.get('mean_accuracy', 0)*100:.2f}%\n",
    "    Std Accuracy: {metrics.get('std_accuracy', 0)*100:.2f}%\n",
    "    \n",
    "    Participants:\n",
    "    Honest: {metrics.get('active_honest', 0)}\n",
    "    Adversarial: {metrics.get('active_adversarial', 0)}\n",
    "    Adversarial Ratio: {metrics.get('adversarial_ratio', 0)*100:.1f}%\n",
    "    \n",
    "    Rounds: {metrics.get('rounds_completed', 0)}\n",
    "    \"\"\"\n",
    "    ax8.text(0.1, 0.5, summary_text, fontfamily='monospace', fontsize=9, \n",
    "             verticalalignment='center', transform=ax8.transAxes)\n",
    "    ax8.axis('off')\n",
    "    \n",
    "    # 9. Metric distribution\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    all_metrics = []\n",
    "    metric_labels = []\n",
    "    if server.accuracy_history:\n",
    "        all_metrics.append(server.accuracy_history)\n",
    "        metric_labels.append('Accuracy')\n",
    "    if server.precision_history:\n",
    "        all_metrics.append(server.precision_history)\n",
    "        metric_labels.append('Precision')\n",
    "    if server.recall_history:\n",
    "        all_metrics.append(server.recall_history)\n",
    "        metric_labels.append('Recall')\n",
    "    if server.f1_history:\n",
    "        all_metrics.append(server.f1_history)\n",
    "        metric_labels.append('F1')\n",
    "    \n",
    "    if all_metrics:\n",
    "        ax9.boxplot(all_metrics, labels=metric_labels)\n",
    "        ax9.set_ylabel('Score')\n",
    "        ax9.set_title('Metric Distribution Across Rounds')\n",
    "        ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Improved FedAvg - {scenario_name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/improved_fedavg_{scenario_name.lower().replace(\" \", \"_\")}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nPlots saved to {save_dir}/improved_fedavg_{scenario_name.lower().replace(' ', '_')}.png\")\n",
    "\n",
    "def print_detailed_performance(metrics, scenario_name):\n",
    "    \"\"\"Print detailed performance metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"IMPROVED FEDAVG - DETAILED PERFORMANCE ({scenario_name})\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nCLASSIFICATION METRICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Accuracy:':<20} {metrics.get('accuracy', 0)*100:>6.2f}%\")\n",
    "    print(f\"{'Precision:':<20} {metrics.get('precision', 0)*100:>6.2f}%\")\n",
    "    print(f\"{'Recall:':<20} {metrics.get('recall', 0)*100:>6.2f}%\")\n",
    "    print(f\"{'F1-Score:':<20} {metrics.get('f1_score', 0)*100:>6.2f}%\")\n",
    "    #print(f\"{'AUC-ROC:':<20} {metrics.get('auc_roc', 0)*100:>6.2f}%\")\n",
    "    print(f\"{'Optimal Threshold:':<20} {metrics.get('optimal_threshold', 0.5):>6.3f}\")\n",
    "    \n",
    "    print(\"\\nSTATISTICAL SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Final Accuracy:':<20} {metrics.get('final_accuracy', 0)*100:>6.2f}%\")\n",
    "    print(f\"{'Max Accuracy:':<20} {metrics.get('max_accuracy', 0)*100:>6.2f}%\")\n",
    "    print(f\"{'Mean Accuracy:':<20} {metrics.get('mean_accuracy', 0)*100:>6.2f}%\")\n",
    "    print(f\"{'Std Accuracy:':<20} {metrics.get('std_accuracy', 0)*100:>6.2f}%\")\n",
    "    print(f\"{'Avg Loss:':<20} {metrics.get('avg_loss', 0):>6.4f}\")\n",
    "    \n",
    "    print(\"\\nPARTICIPANT COMPOSITION:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Active Honest:':<20} {metrics.get('active_honest', 0)}\")\n",
    "    print(f\"{'Active Adversarial:':<20} {metrics.get('active_adversarial', 0)}\")\n",
    "    print(f\"{'Total Active:':<20} {metrics.get('total_active', 0)}\")\n",
    "    print(f\"{'Adversarial Ratio:':<20} {metrics.get('adversarial_ratio', 0)*100:>6.1f}%\")\n",
    "    \n",
    "    print(\"\\nCONFUSION MATRIX:\")\n",
    "    print(\"-\" * 50)\n",
    "    if metrics.get('confusion_matrix'):\n",
    "        cm = np.array(metrics['confusion_matrix'])\n",
    "        print(f\"                Predicted\")\n",
    "        print(f\"                Honest   Adversarial\")\n",
    "        print(f\"Actual Honest    {cm[0,0]:^8} {cm[0,1]:^11}\")\n",
    "        print(f\"Actual Adversarial {cm[1,0]:^8} {cm[1,1]:^11}\")\n",
    "        print(\"\\nCONFUSION MATRIX (SIMPLIFIED):\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"True Positives: {cm[1,1]}\")\n",
    "        print(f\"False Positives: {cm[0,1]}\")\n",
    "        print(f\"True Negatives: {cm[0,0]}\")\n",
    "        print(f\"False Negatives: {cm[1,0]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# ===== MAIN SIMULATION =====\n",
    "def run_fedavg_simulation(dataset_path, num_honest, num_adversarial, rounds=35, device='cpu', save_dir='.'):\n",
    "    \"\"\"Main FedAvg simulation function\"\"\"\n",
    "    set_full_determinism()\n",
    "    \n",
    "    scenario_name = \"Honest Majority\" if num_honest > num_adversarial else \"Adversarial Majority\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"IMPROVED FEDAVG - {scenario_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nLoading dataset from {dataset_path}...\")\n",
    "    dataset = V2GDataset(dataset_path, use_augmentation=True, use_smote=True)\n",
    "    \n",
    "    # Get class weights\n",
    "    class_weights = dataset.get_class_weights()\n",
    "    print(f\"\\nClass weights: {class_weights.numpy()}\")\n",
    "\n",
    "    global_model = V2GClassifier(input_size=dataset.num_features, num_classes=dataset.num_classes).to(device)\n",
    "    server = FedAvgServer(global_model, num_honest, num_adversarial, device, dataset, class_weights)\n",
    "\n",
    "    # Stratified train-val split\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        indices, \n",
    "        test_size=0.2, \n",
    "        stratify=dataset.labels.numpy(), \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain size: {len(train_indices)}, Val size: {len(val_indices)}\")\n",
    "    \n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, \n",
    "                           generator=data_loader_generator, worker_init_fn=seed_worker_fn)\n",
    "\n",
    "    print(\"\\nDistributing data to participants...\")\n",
    "    activated_honest, activated_adversarial = distribute_data_to_participants_fedavg(server, dataset, train_indices)\n",
    "\n",
    "    print(f\"\\nStarting Improved FedAvg:\")\n",
    "    print(f\"  • {activated_honest} honest, {activated_adversarial} adversarial\")\n",
    "    print(f\"  • Total rounds: {rounds}\")\n",
    "    print(f\"  • Improvements: Reduced model size, class weights, threshold optimization\")\n",
    "    \n",
    "    # Initialize all participant models\n",
    "    for pid in server.active_participants:\n",
    "        server.participants[pid]['model'].load_state_dict(global_model.state_dict())\n",
    "    \n",
    "    # Training loop\n",
    "    for round_idx in range(1, rounds + 1):\n",
    "        server.current_round = round_idx\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Round {round_idx}/{rounds}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        selected_pids = server.select_participants(fraction=0.5)\n",
    "        print(f\"Selected {len(selected_pids)} participants\")\n",
    "        \n",
    "        # Verify scenario difference\n",
    "        selected_honest = sum(1 for pid in selected_pids if server.ground_truth_types[pid] == 'honest')\n",
    "        selected_adv = len(selected_pids) - selected_honest\n",
    "        print(f\"  Selected: {selected_honest}H / {selected_adv}A\")\n",
    "        \n",
    "        # Train selected participants\n",
    "        for pid in selected_pids:\n",
    "            if not server.participants[pid]['active']:\n",
    "                continue\n",
    "                \n",
    "            is_adversarial = server.ground_truth_types[pid] == 'adversarial'\n",
    "            \n",
    "            loss, accuracy = train_participant_fedavg(\n",
    "                server.participants[pid], \n",
    "                dataset, \n",
    "                device, \n",
    "                round_idx, \n",
    "                pid, \n",
    "                is_adversarial,\n",
    "                class_weights\n",
    "            )\n",
    "            \n",
    "            type_str = \"ADV\" if is_adversarial else \"HON\"\n",
    "            print(f\"  P{pid} ({type_str}): Loss={loss:.4f}, Acc={accuracy:.3f}\")\n",
    "        \n",
    "        # Federated averaging\n",
    "        if selected_pids:\n",
    "            server.federated_averaging(selected_pids)\n",
    "            \n",
    "            # Update all participant models\n",
    "            for pid in server.active_participants:\n",
    "                server.participants[pid]['model'].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy, precision, recall, f1, auc_roc, avg_loss, cm = server.evaluate_model(val_loader)\n",
    "        \n",
    "        server.accuracy_history.append(accuracy)\n",
    "        server.precision_history.append(precision)\n",
    "        server.recall_history.append(recall)\n",
    "        server.f1_history.append(f1)\n",
    "        server.loss_history.append(avg_loss)\n",
    "        server.confusion_matrices.append(cm)\n",
    "        \n",
    "        metrics = server.get_metrics()\n",
    "        print(f\"\\nGlobal Model:\")\n",
    "        print(f\"  Acc: {accuracy:.3f} | Prec: {precision:.3f} | Rec: {recall:.3f} | F1: {f1:.3f}\")\n",
    "        print(f\"  Loss: {avg_loss:.4f} | Active: {metrics['total_active']} ({metrics['active_honest']}H/{metrics['active_adversarial']}A)\")\n",
    "        \n",
    "        # Diagnose predictions every 5 rounds\n",
    "        if round_idx % 5 == 0 or round_idx == 1:\n",
    "            server.diagnose_predictions(val_loader)\n",
    "        \n",
    "        # Early stopping\n",
    "        if len(server.accuracy_history) >= 10:\n",
    "            recent_acc = server.accuracy_history[-10:]\n",
    "            if max(recent_acc) - min(recent_acc) < 0.005:\n",
    "                print(f\"\\nEarly stopping: Accuracy plateaued at round {round_idx}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Training Complete\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    final_metrics = calculate_comprehensive_metrics(server, val_loader)\n",
    "    create_comprehensive_plots(server, final_metrics, scenario_name, save_dir)\n",
    "    print_detailed_performance(final_metrics, scenario_name)\n",
    "    \n",
    "    metrics_file = f'{save_dir}/improved_fedavg_metrics_{scenario_name.lower().replace(\" \", \"_\")}.json'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(final_metrics, f, indent=4, default=str)\n",
    "    print(f\"\\nMetrics saved to: {metrics_file}\")\n",
    "    \n",
    "    return server, final_metrics\n",
    "\n",
    "def compare_scenarios(metrics_honest, metrics_adv):\n",
    "    \"\"\"Compare performance between scenarios\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SCENARIO COMPARISON: Honest vs Adversarial Majority\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n{'Metric':<25} {'Honest Majority':>20} {'Adversarial Majority':>20} {'Difference':>15}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    metrics_to_compare = [\n",
    "        ('Accuracy', 'accuracy'),\n",
    "        ('Precision', 'precision'),\n",
    "        ('Recall', 'recall'),\n",
    "        ('F1-Score', 'f1_score'),\n",
    "        ('AUC-ROC', 'auc_roc')\n",
    "    ]\n",
    "    \n",
    "    for display_name, metric_key in metrics_to_compare:\n",
    "        hon_val = metrics_honest.get(metric_key, 0) * 100\n",
    "        adv_val = metrics_adv.get(metric_key, 0) * 100\n",
    "        diff = adv_val - hon_val\n",
    "        diff_sign = '+' if diff >= 0 else ''\n",
    "        \n",
    "        print(f\"{display_name:<25} {hon_val:>18.2f}% {adv_val:>18.2f}% {diff_sign}{diff:>13.2f}%\")\n",
    "    \n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    if metrics_honest.get('accuracy', 0) > 0:\n",
    "        degradation = (metrics_honest['accuracy'] - metrics_adv['accuracy']) / metrics_honest['accuracy'] * 100\n",
    "        print(f\"\\nPerformance Degradation: {degradation:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cpu'\n",
    "    dataset_path = r\"C:\\Users\\Administrator\\Desktop\\v2g dataset kaggle.csv\"\n",
    "    \n",
    "    os.makedirs('./improved_honest_majority', exist_ok=True)\n",
    "    os.makedirs('./improved_adversarial_majority', exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPROVED FEDAVG WITH ALL ENHANCEMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nImprovements:\")\n",
    "    print(\"  ✓ Reduced model capacity (256 vs 512 hidden)\")\n",
    "    print(\"  ✓ Class-weighted CrossEntropyLoss\")\n",
    "    print(\"  ✓ Minimal data augmentation (0.02 noise)\")\n",
    "    print(\"  ✓ Less aggressive SMOTE (0.8 ratio)\")\n",
    "    print(\"  ✓ Reduced adversarial poisoning (20% → 10%)\")\n",
    "    print(\"  ✓ Optimal threshold tuning\")\n",
    "    print(\"  ✓ Stratified train-val split\")\n",
    "    print(\"  ✓ BatchNorm + LeakyReLU\")\n",
    "    print(\"  ✓ AdamW with weight decay\")\n",
    "    print(\"  ✓ Prediction diagnostics\")\n",
    "    \n",
    "    # Honest Majority\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SCENARIO 1: HONEST MAJORITY (10 Honest, 9 Adversarial)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    server_honest, metrics_honest = run_fedavg_simulation(\n",
    "        dataset_path, num_honest=10, num_adversarial=9, rounds=15, device=device,\n",
    "        save_dir='./improved_honest_majority'\n",
    "    )\n",
    "    \n",
    "    # Adversarial Majority\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SCENARIO 2: ADVERSARIAL MAJORITY (10 Honest, 11 Adversarial)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    server_adv, metrics_adv = run_fedavg_simulation(\n",
    "        dataset_path, num_honest=10, num_adversarial=11, rounds=15, device=device,\n",
    "        save_dir='./improved_adversarial_majority'\n",
    "    )\n",
    "    \n",
    "    # Compare\n",
    "    compare_scenarios(metrics_honest, metrics_adv)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n✓ Honest Majority:\")\n",
    "    print(f\"  Accuracy: {metrics_honest.get('accuracy', 0)*100:.1f}%\")\n",
    "    print(f\"  F1-Score: {metrics_honest.get('f1_score', 0)*100:.1f}%\")\n",
    "    print(f\"  Precision: {metrics_honest.get('precision', 0)*100:.1f}%\")\n",
    "    print(f\"  Recall: {metrics_honest.get('recall', 0)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n✓ Adversarial Majority:\")\n",
    "    print(f\"  Accuracy: {metrics_adv.get('accuracy', 0)*100:.1f}%\")\n",
    "    print(f\"  F1-Score: {metrics_adv.get('f1_score', 0)*100:.1f}%\")\n",
    "    print(f\"  Precision: {metrics_adv.get('precision', 0)*100:.1f}%\")\n",
    "    print(f\"  Recall: {metrics_adv.get('recall', 0)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nResults saved in:\")\n",
    "    print(f\"  • ./improved_honest_majority/\")\n",
    "    print(f\"  • ./improved_adversarial_majority/\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
