import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score
from torch.utils.data import Dataset, DataLoader, Subset
from imblearn.over_sampling import SMOTE
import random
from collections import defaultdict
import matplotlib.pyplot as plt
import os

# Custom Focal Loss to improve recall for adversarial class
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.90, gamma=2.5):  # Increased alpha even more to prioritize adversarial class
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, outputs, targets):
        nll_loss = nn.NLLLoss(reduction='none')(outputs, targets)
        pt = torch.exp(-nll_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * nll_loss
        return focal_loss.mean()

# Custom Dataset for V2G
class V2GDataset(Dataset):
    def __init__(self, csv_path):
        self.data = pd.read_csv(csv_path)
        print(f"Dataset loaded with {len(self.data)} records")
        print(f"Columns: {', '.join(self.data.columns)}")
        
        # Preprocess timestamp
        if 'timestamp' in self.data.columns:
            self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])
            self.data['hour'] = self.data['timestamp'].dt.hour
            # Add more time-based features
            self.data['day_of_week'] = self.data['timestamp'].dt.dayofweek
            self.data['is_weekend'] = (self.data['day_of_week'] >= 5).astype(int)
        else:
            self.data['hour'] = 0
            self.data['day_of_week'] = 0
            self.data['is_weekend'] = 0
        
        # Enhanced feature engineering 
        self.data['charge_discharge_ratio'] = self.data['current_charge_kWh'] / (self.data['discharge_rate_kW'] + 1e-6)
        self.data['energy_hour_interaction'] = self.data['energy_requested_kWh'] * self.data['hour']
        self.data['charge_capacity_ratio'] = self.data['current_charge_kWh'] / (self.data['battery_capacity_kWh'] + 1e-6)
        self.data['efficiency_estimate'] = self.data['energy_requested_kWh'] / (self.data['discharge_rate_kW'] * 0.25 + 1e-6)
        
        # Features, labels, and participant IDs
        feature_cols = ['battery_capacity_kWh', 'current_charge_kWh', 'discharge_rate_kW', 
                       'energy_requested_kWh', 'hour', 'charge_discharge_ratio', 
                       'energy_hour_interaction', 'charge_capacity_ratio', 
                       'efficiency_estimate', 'day_of_week', 'is_weekend']
        self.features = self.data[feature_cols].values
        self.labels = self.data['label'].str.lower().values
        self.participant_ids = self.data['participant_id'].values
        self.original_indices = np.arange(len(self.data))  # Track original indices
        
        # Compute participant statistics before SMOTE
        self.label_encoder = LabelEncoder()
        original_labels = self.label_encoder.fit_transform(self.labels)
        print(f"Label classes: {', '.join(self.label_encoder.classes_)}")
        
        unique_participants = len(set(self.participant_ids))
        honest_count = sum(original_labels == self.label_encoder.transform(['honest'])[0])
        adversarial_count = len(original_labels) - honest_count
        print(f"Data distribution: {honest_count} honest, {adversarial_count} adversarial records")
        
        honest_mask = original_labels == self.label_encoder.transform(['honest'])[0]
        honest_parts = len(set(self.participant_ids[honest_mask]))
        adversarial_parts = unique_participants - honest_parts
        print(f"Participant distribution: {honest_parts} honest, {adversarial_parts} adversarial participants")
        
        # Apply SMOTE to balance classes - using default strategy to avoid error
        smote = SMOTE(random_state=42)  # Default strategy balances all classes to majority class
        self.features, self.labels = smote.fit_resample(self.features, self.labels)
        self.labels = self.label_encoder.transform(self.labels)
        
        # Adjust class weights in other components rather than through sampling imbalance
        
        # Scale features
        self.scaler = StandardScaler()
        self.features = self.scaler.fit_transform(self.features)
        
        # Dataset stats
        self.num_features = self.features.shape[1]
        self.num_classes = len(self.label_encoder.classes_)
        print(f"Features: {self.num_features}, Classes: {self.num_classes}")
        print(f"Dataset participant IDs: {sorted(set(self.participant_ids))}")
        
        # Convert to tensors
        self.features = torch.tensor(self.features, dtype=torch.float32)
        self.labels = torch.tensor(self.labels, dtype=torch.long)
        
        # Group indices by participant (using original indices)
        self.participant_indices = defaultdict(list)
        for idx, pid in enumerate(self.participant_ids):
            self.participant_indices[pid].append(idx)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]
    
    def get_participant_data(self, participant_id):
        indices = self.participant_indices.get(participant_id, [])
        return self.features[indices], self.labels[indices]
    
    def get_participant_indices(self, participant_id):
        return self.participant_indices.get(participant_id, [])
    
    def get_participant_type(self, participant_id):
        indices = self.get_participant_indices(participant_id)
        if not indices:
            return None
        labels = self.labels[indices].numpy()
        return 'honest' if np.mean(labels == self.label_encoder.transform(['honest'])[0]) > 0.5 else 'adversarial'
    
    def get_original_indices(self):
        return self.original_indices

# Enhanced Neural Network Model
class V2GClassifier(nn.Module):
    def __init__(self, input_size, hidden_size=512, num_classes=2, dropout=0.35):
        super(V2GClassifier, self).__init__()
        self.input_layer = nn.Linear(input_size, hidden_size)
        self.norm1 = nn.LayerNorm(hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        
        # Residual block 1
        self.residual1 = nn.Linear(hidden_size, hidden_size // 2)
        self.norm2 = nn.LayerNorm(hidden_size // 2)
        self.residual2 = nn.Linear(hidden_size // 2, hidden_size // 2)
        self.norm3 = nn.LayerNorm(hidden_size // 2)
        
        # Additional residual block
        self.residual3 = nn.Linear(hidden_size // 2, hidden_size // 4)
        self.norm4 = nn.LayerNorm(hidden_size // 4)
        self.residual4 = nn.Linear(hidden_size // 4, hidden_size // 4)
        self.norm5 = nn.LayerNorm(hidden_size // 4)
        
        # Feature interaction
        self.interaction = nn.Sequential(
            nn.Linear(hidden_size // 4, hidden_size // 8),
            nn.ReLU(),
            nn.Linear(hidden_size // 8, hidden_size // 4)
        )
        
        # Attention mechanism for feature importance
        self.attention = nn.Sequential(
            nn.Linear(hidden_size // 4, hidden_size // 8),
            nn.Tanh(),
            nn.Linear(hidden_size // 8, 1),
            nn.Softmax(dim=1)
        )
        
        self.output_layer = nn.Linear(hidden_size // 4, num_classes)
        self.log_softmax = nn.LogSoftmax(dim=1)
    
    def forward(self, x):
        x = self.input_layer(x)
        x = self.norm1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # Residual block 1
        residual = x
        x = self.residual1(x)
        x = self.norm2(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.residual2(x)
        x = self.norm3(x)
        x = self.relu(x)
        
        # Residual block 2
        residual = x
        x = self.residual3(x)
        x = self.norm4(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.residual4(x)
        x = self.norm5(x)
        x = self.relu(x)
        
        x = x + self.interaction(x)
        x = self.dropout(x)
        
        # Apply attention mechanism
        if hasattr(self, 'attention'):
            attn_weights = self.attention(x.unsqueeze(1)).squeeze(-1)
            x = x * attn_weights
        
        x = self.output_layer(x)
        return self.log_softmax(x)

# Parameters Class for Dynamic Adaptation
class AdaptiveParameters:
    def __init__(self, num_honest, num_adversarial, adaptive_mode=True):
        self.num_honest = num_honest
        self.num_adversarial = num_adversarial
        self.adversarial_majority = num_adversarial > num_honest
        self.adaptive_mode = adaptive_mode
        self.current_detected_ratio = 0.5
        
        # Initialize base parameters
        self.init_parameters()
        
    def init_parameters(self):
        self.num_byzantines = max(3, int(self.num_adversarial * 0.3))
        total_participants = self.num_honest + self.num_adversarial
        self.multi_krum_m = max(3, int(total_participants * 0.4))
        
        # DRAMATICALLY LOWERED THRESHOLDS FOR HIGHER RECALL
        self.removal_threshold_base = 1.2  # Drastically reduced from 1.8
        self.max_removal_rate_base = 0.25  # Significantly increased from 0.18
        self.bootstrap_rounds_base = 2  # Minimal bootstrap to detect adversaries very early
        self.whitelist_rounds_base = 8  # Increased to be more cautious about safe-listing
        self.confidence_threshold_base = 0.20  # Dramatically reduced from 0.50
        self.reputation_decay_base = 0.80  # Much more aggressive reputation decay
        self.honest_protection_factor_base = 5.0  # Higher protection for honest nodes
        self.forced_removal_base = True  # Always enable forced removal
        self.min_accuracy_threshold_base = 0.50  # Relaxed threshold to trigger more removals
        self.adv_per_round_limit_base = 8  # Drastically increased limit for more aggressive removal
        self.patience_base = 8  # Reduced to stop earlier if not improving
        self.min_rounds_base = 10  # Reduced to allow earlier stopping
        self.suspicion_weight_base = 0.40  # Drastically increased weight for suspicion
        self.selection_factor_weight_base = 0.25
        self.behavior_factor_weight_base = 0.30
        self.consecutive_factor_weight_base = 0.05
        self.reputation_factor_weight_base = 0.00  # Removed impact of reputation on confidence
        
        # Even more aggressive parameters for adversarial scenario
        self.removal_threshold_adv = 1.0  # Almost no accumulation needed
        self.max_removal_rate_adv = 0.40  # Extremely aggressive removal rate
        self.bootstrap_rounds_adv = 1  # Minimal bootstrap
        self.whitelist_rounds_adv = 25  # Very cautious about safe-listing
        self.confidence_threshold_adv = 0.25  # Extremely low threshold
        self.reputation_decay_adv = 0.70  # Extremely aggressive reputation decay
        self.honest_protection_factor_adv = 10.0  # Much higher protection for honest nodes
        self.forced_removal_adv = True
        self.min_accuracy_threshold_adv = 0.40  # Very low to trigger removals
        self.adv_per_round_limit_adv = 10  # Remove many participants per round
        self.patience_adv = 8
        self.min_rounds_adv = 20
        self.suspicion_weight_adv = 0.50  # Half of confidence from suspicion alone
        self.selection_factor_weight_adv = 0.30
        self.behavior_factor_weight_adv = 0.20
        self.consecutive_factor_weight_adv = 0.00  # Removed impact
        self.reputation_factor_weight_adv = 0.00  # Removed impact
        self.update_parameters(self.adversarial_majority)
        
        # Enhanced protection settings
        self.honest_removal_budget = int(self.num_honest * 0.25)  # Reduced budget to protect more honest participants
        self.safe_list_selection_threshold = 0.4  # Increased to be more selective about safe-listing
        self.safe_list_reputation_threshold = 0.75  # Increased to be more selective
        self.gradient_cv_threshold = 0.30  # Lowered to be more sensitive to gradient variations
        self.behavioral_flag_threshold = 1.8  # Lowered to flag suspicious behavior earlier
        self.reputation_penalty = 0.18  # Increased penalty
        self.min_selection_rate = 0.12  # Lowered to be less restrictive
        
        # Honest identification thresholds
        self.honest_selection_rate_threshold = 0.48  # Increased for stricter honest classification
        self.honest_reputation_threshold = 0.88  # Increased for stricter honest classification
        self.honest_gradient_cv_threshold = 0.22  # Decreased for stricter honest classification
        self.honest_accuracy_threshold = 0.58  # Increased for stricter honest classification
        
        # Adaptation parameters
        self.update_factor = 0.15  # Increased for faster adaptation
        self.max_adaptation = 0.85  # Increased for more flexibility
    
    def update_parameters(self, adversarial_focus):
        if isinstance(adversarial_focus, bool):
            adv_ratio = 1.0 if adversarial_focus else 0.0
        else:
            adv_ratio = min(max(adversarial_focus, 0.0), 1.0)
        
        self.removal_threshold = self.removal_threshold_base + adv_ratio * (self.removal_threshold_adv - self.removal_threshold_base)
        self.max_removal_rate = self.max_removal_rate_base + adv_ratio * (self.max_removal_rate_adv - self.max_removal_rate_base)
        self.bootstrap_rounds = int(self.bootstrap_rounds_base + adv_ratio * (self.bootstrap_rounds_adv - self.bootstrap_rounds_base))
        self.whitelist_rounds = int(self.whitelist_rounds_base + adv_ratio * (self.whitelist_rounds_adv - self.whitelist_rounds_base))
        self.confidence_threshold = self.confidence_threshold_base + adv_ratio * (self.confidence_threshold_adv - self.confidence_threshold_base)
        self.reputation_decay = self.reputation_decay_base + adv_ratio * (self.reputation_decay_adv - self.reputation_decay_base)
        self.honest_protection_factor = self.honest_protection_factor_base + adv_ratio * (self.honest_protection_factor_adv - self.honest_protection_factor_base)
        self.forced_removal = True if adv_ratio > 0.3 else self.forced_removal_base  # Enable forced removal at lower threshold
        self.min_accuracy_threshold = self.min_accuracy_threshold_base + adv_ratio * (self.min_accuracy_threshold_adv - self.min_accuracy_threshold_base)
        self.adv_per_round_limit = int(self.adv_per_round_limit_base + adv_ratio * (self.adv_per_round_limit_adv - self.adv_per_round_limit_base))
        self.patience = int(self.patience_base + adv_ratio * (self.patience_adv - self.patience_base))
        self.min_rounds = int(self.min_rounds_base + adv_ratio * (self.min_rounds_adv - self.min_rounds_base))
        
        self.suspicion_weight = self.suspicion_weight_base + adv_ratio * (self.suspicion_weight_adv - self.suspicion_weight_base)
        self.selection_factor_weight = self.selection_factor_weight_base + adv_ratio * (self.selection_factor_weight_adv - self.selection_factor_weight_base)
        self.behavior_factor_weight = self.behavior_factor_weight_base + adv_ratio * (self.behavior_factor_weight_adv - self.behavior_factor_weight_base)
        self.consecutive_factor_weight = self.consecutive_factor_weight_base + adv_ratio * (self.consecutive_factor_weight_adv - self.consecutive_factor_weight_base)
        self.reputation_factor_weight = self.reputation_factor_weight_base + adv_ratio * (self.reputation_factor_weight_adv - self.reputation_factor_weight_base)
    
    def adapt_to_environment(self, metric_data):
        if not self.adaptive_mode:
            return
        
        avg_grad_cv = metric_data.get('avg_gradient_cv', 0.5)
        selection_variance = metric_data.get('selection_variance', 0.5)
        recent_accuracy = metric_data.get('recent_accuracy', 0.6)
        confidence_disparity = metric_data.get('confidence_disparity', 0.5)
        
        # Enhanced environment assessment with more weight on gradient CV and selection variance
        environment_assessment = (
            (avg_grad_cv > 0.30) * 0.40 +  # Increased weight and lower threshold
            (selection_variance > 0.20) * 0.30 +  # Increased weight and lower threshold
            (recent_accuracy < 0.55) * 0.20 +  # Lower threshold
            (confidence_disparity > 0.20) * 0.10   # Lower threshold
        )
        
        self.current_detected_ratio = (1 - self.update_factor) * self.current_detected_ratio + self.update_factor * environment_assessment
        
        mid_point = 0.5
        max_adaptation = self.max_adaptation
        if self.adversarial_majority:
            self.current_detected_ratio = max(mid_point, min(1.0, self.current_detected_ratio))
        else:
            self.current_detected_ratio = min(max(0.0, self.current_detected_ratio), mid_point + max_adaptation)
        
        self.update_parameters(self.current_detected_ratio)
        
    def get_status(self):
        return {
            "detected_ratio": self.current_detected_ratio,
            "removal_threshold": self.removal_threshold,
            "confidence_threshold": self.confidence_threshold,
            "max_removal_rate": self.max_removal_rate,
            "forced_removal": self.forced_removal,
            "behavior_weight": self.behavior_factor_weight,
            "selection_weight": self.selection_factor_weight
        }

# Improved Federated Server with Enhanced Recall
class ImprovedFederatedServer:
    def __init__(self, model, num_honest, num_adversarial, device='cpu', dataset=None, adaptive_mode=True):
        self.model = model.to(device)
        self.device = device
        self.num_honest = num_honest
        self.num_adversarial = num_adversarial
        self.dataset = dataset
        self.current_round = 0
        self.participants = {}
        self.reputation_scores = {}
        self.confidence_scores = {}
        self.gradient_norms = defaultdict(list)
        self.gradient_directions = defaultdict(list)
        self.selection_history = defaultdict(list)
        self.accuracy_history = []
        self.removed_participants = []
        self.safe_list = set()
        self.honest_removed_count = 0
        
        self.selection_rates = defaultdict(float)
        self.gradient_cvs = defaultdict(float)
        self.confidence_history = defaultdict(list)
        self.suspicious_patterns = defaultdict(int)
        self.accuracy_improvements = []
        self.parameter_history = []
        
        # New metrics for enhanced detection
        self.gradient_cosine_similarities = defaultdict(list)  # Track cosine similarity between gradients
        self.gradient_deviations = defaultdict(float)  # Track deviations from average honest gradient
        self.participation_consistency = defaultdict(float)  # Track consistency in participation
        self.historical_suspicion = defaultdict(list)  # Track suspicion score history
        self.last_selected_round = defaultdict(int)  # Track last round participant was selected
        
        self.params = AdaptiveParameters(num_honest, num_adversarial, adaptive_mode)
        
        print(f"{'ADVERSARIAL MAJORITY DETECTED' if self.params.adversarial_majority else 'HONEST MAJORITY DETECTED'} - Using {'adaptive' if adaptive_mode else 'fixed'} strategy")
        print(f"Detection params: threshold={self.params.removal_threshold:.2f}, rate={self.params.max_removal_rate:.2f}")
        print(f"Multi-Krum params: byzantines={self.params.num_byzantines}, m={self.params.multi_krum_m}")
        print(f"Honest protection: budget={self.params.honest_removal_budget}, factor={self.params.honest_protection_factor:.2f}")
        
        self._initialize_participants(dataset)
    
    def _initialize_participants(self, dataset):
        available_pids = list(set(dataset.participant_ids))
        if len(available_pids) < self.num_honest + self.num_adversarial:
            print(f"Warning: Only {len(available_pids)} participants available, adjusting counts")
            total = self.num_honest + self.num_adversarial
            self.num_honest = int(len(available_pids) * (self.num_honest / total))
            self.num_adversarial = len(available_pids) - self.num_honest
            print(f"Adjusted to {self.num_honest} honest and {self.num_adversarial} adversarial participants")
        
        random.shuffle(available_pids)
        honest_ids = available_pids[:self.num_honest]
        adversarial_ids = available_pids[self.num_honest:self.num_honest + self.num_adversarial]
        
        for idx in honest_ids:
            self.participants[idx] = {
                'model': V2GClassifier(self.model.input_layer.in_features, num_classes=self.model.output_layer.out_features).to(self.device),
                'type': 'honest',
                'data': None,
                'active': True,
                'suspicion_score': 0.0,
                'behavioral_flags': 0,
                'selection_count': 0,
                'gradient_history': [],
                'accuracy_history': [],
                'detected_anomalies': 0,
                'participation_rate': 1.0,
                'consistent_behavior_score': 1.0,
                'rounds_since_selection': 0
            }
            self.reputation_scores[idx] = 1.0
            self.confidence_scores[idx] = 0.0
        
        for idx in adversarial_ids:
            self.participants[idx] = {
                'model': V2GClassifier(self.model.input_layer.in_features, num_classes=self.model.output_layer.out_features).to(self.device),
                'type': 'adversarial',
                'data': None,
                'active': True,
                'suspicion_score': 0.0,
                'behavioral_flags': 0,
                'selection_count': 0,
                'gradient_history': [],
                'accuracy_history': [],
                'detected_anomalies': 0,
                'participation_rate': 1.0,
                'consistent_behavior_score': 1.0,
                'rounds_since_selection': 0
            }
            self.reputation_scores[idx] = 1.0
            self.confidence_scores[idx] = 0.0
        
        print(f"Server initialized with {self.num_honest} honest and {self.num_adversarial} adversarial participants")
    
    def calculate_gradient_metrics(self, idx, gradients):
        # Convert gradients to a flat vector
        grad_vector = torch.cat([g.flatten() for g in gradients]).to(self.device)
        norm = torch.norm(grad_vector).item()
        self.gradient_norms[idx].append(norm)
        
        # Store gradient history
        if len(self.participants[idx]['gradient_history']) >= 5:
            self.participants[idx]['gradient_history'].pop(0)
        self.participants[idx]['gradient_history'].append(norm)
        
        # Calculate coefficient of variation of gradients
        if len(self.gradient_norms[idx]) >= 2:  # Reduced from 3 to 2 for earlier detection
            norms = self.gradient_norms[idx][-5:] if len(self.gradient_norms[idx]) >= 5 else self.gradient_norms[idx]
            variance = np.var(norms)
            mean_norm = np.mean(norms)
            cv = np.sqrt(variance) / (mean_norm + 1e-10)
            self.gradient_cvs[idx] = cv
            
            # MUCH more aggressive flagging if CV is high
            if cv > self.params.gradient_cv_threshold:
                self.participants[idx]['behavioral_flags'] += 2.0  # Doubled penalty
                self.suspicious_patterns[idx] += 2.0
            
            # Add suspicion even for moderate CV values
            elif cv > self.params.gradient_cv_threshold * 0.7:
                self.participants[idx]['behavioral_flags'] += 0.8
                self.suspicious_patterns[idx] += 0.8
        
        # Enhanced gradient analysis with cosine similarity - MUCH more aggressive penalties
        if len(self.gradient_directions[idx]) >= 1:
            prev_grad = torch.cat([g.flatten() for g in self.gradient_directions[idx][-1]]).to(self.device)
            
            # Calculate cosine similarity
            direction_change = torch.dot(grad_vector / (norm + 1e-10), prev_grad / (torch.norm(prev_grad) + 1e-10)).item()
            self.gradient_cosine_similarities[idx].append(direction_change)
            
            # Calculate magnitude difference
            magnitude_diff = abs(norm - torch.norm(prev_grad).item()) / (torch.norm(prev_grad).item() + 1e-10)
            
            # DRASTICALLY increase penalties for suspicious patterns
            if direction_change < 0.03:  # Very low similarity
                self.participants[idx]['behavioral_flags'] += 10.0  # Increased from 7
                self.suspicious_patterns[idx] += 10.0
                # Immediately add suspicion score for very suspicious behaviors
                self.participants[idx]['suspicion_score'] += 0.5
            elif direction_change < 0.1:  # Medium similarity threshold
                self.participants[idx]['behavioral_flags'] += 5.0  # Increased from 3
                self.suspicious_patterns[idx] += 5.0
                self.participants[idx]['suspicion_score'] += 0.3
            elif direction_change > 0.98:  # Very high similarity is also suspicious
                self.participants[idx]['behavioral_flags'] += 2.0  # Increased from 0.9
                self.suspicious_patterns[idx] += 2.0
                self.participants[idx]['suspicion_score'] += 0.2
                
            if magnitude_diff > 0.4:  # Large magnitude difference
                self.participants[idx]['behavioral_flags'] += 4.0  # Increased from 2.5
                self.suspicious_patterns[idx] += 4.0
                self.participants[idx]['suspicion_score'] += 0.25
            elif magnitude_diff > 0.2:  # Medium magnitude difference
                self.participants[idx]['behavioral_flags'] += 2.0  # Increased from 1.0
                self.suspicious_patterns[idx] += 2.0
                self.participants[idx]['suspicion_score'] += 0.15
        
        # Track time since last selection - penalize consistently non-selected participants
        self.participants[idx]['rounds_since_selection'] += 1
        if self.participants[idx]['rounds_since_selection'] > 3:
            self.participants[idx]['behavioral_flags'] += 0.5 * (self.participants[idx]['rounds_since_selection'] - 3)
        
        self.gradient_directions[idx].append([g.clone().detach() for g in gradients])
        
        # Calculate deviation from average honest gradient (if we have identified some honest participants)
        honest_candidates = [pid for pid in self.safe_list if self.participants[pid]['active']]
        if honest_candidates and len(self.gradient_directions[idx]) > 0:
            avg_honest_grad = None
            honest_count = 0
            
            # Calculate average honest gradient
            for hid in honest_candidates:
                if hid in self.gradient_directions and self.gradient_directions[hid]:
                    h_grad = torch.cat([g.flatten() for g in self.gradient_directions[hid][-1]]).to(self.device)
                    if avg_honest_grad is None:
                        avg_honest_grad = h_grad
                    else:
                        avg_honest_grad += h_grad
                    honest_count += 1
            
            if honest_count > 0 and avg_honest_grad is not None:
                avg_honest_grad = avg_honest_grad / honest_count
                # Calculate cosine similarity with average honest gradient
                cos_sim = torch.dot(grad_vector / (norm + 1e-10), 
                                   avg_honest_grad / (torch.norm(avg_honest_grad) + 1e-10)).item()
                self.gradient_deviations[idx] = 1.0 - cos_sim  # Higher value means more deviation
                
                # Penalize large deviations from honest gradient
                if self.gradient_deviations[idx] > 0.4:  # Significant deviation
                    deviation_penalty = (self.gradient_deviations[idx] - 0.4) * 5.0
                    self.participants[idx]['behavioral_flags'] += deviation_penalty
                    self.suspicious_patterns[idx] += deviation_penalty
    
    def improved_multi_krum(self, gradients_dict):
        if not gradients_dict:
            return [], []
        
        n = len(gradients_dict)
        f = min(self.params.num_byzantines, n // 3)
        m = min(self.params.multi_krum_m, n - f - 2)
        
        if m <= 0:
            m = max(1, n // 2)
        
        distances = {}
        for i in gradients_dict:
            distances[i] = []
            grad_i = torch.cat([g.flatten() for g in gradients_dict[i]]).to(self.device)
            for j in gradients_dict:
                if i != j:
                    grad_j = torch.cat([g.flatten() for g in gradients_dict[j]]).to(self.device)
                    dist = torch.norm(grad_i - grad_j).item()
                    distances[i].append(dist)
        
        krum_scores = {}
        for i in distances:
            sorted_dists = sorted(distances[i])[:n - f - 2]
            base_score = sum(sorted_dists)
            
            if self.current_round > self.params.bootstrap_rounds:
                # MUCH more aggressive reputation modifier
                reputation_modifier = np.clip(2.0 / (self.reputation_scores[i] + 0.1), 0.5, 6.0)  # Increased range
                
                # Additional penalty for suspicious behaviors - much more aggressive
                if i in self.gradient_deviations and self.gradient_deviations[i] > 0.3:
                    reputation_modifier *= (1.0 + self.gradient_deviations[i] * 2.0)  # Doubled impact
                
                # Participants that haven't been selected get penalized much more
                if self.current_round > 5 and self.selection_rates[i] < 0.25:  # Reduced threshold
                    reputation_modifier *= (1.5 + (0.25 - self.selection_rates[i]) * 4.0)  # Doubled impact
                
                krum_scores[i] = base_score * reputation_modifier
            else:
                # Even in bootstrap phase, apply some penalties
                if self.current_round > 1:
                    if i in self.gradient_deviations and self.gradient_deviations[i] > 0.4:
                        base_score *= 1.3
                    if i in self.gradient_cvs and self.gradient_cvs[i] > 0.3:
                        base_score *= 1.2
                krum_scores[i] = base_score
        
        selected = sorted(krum_scores.items(), key=lambda x: x[1])[:m]
        selected_ids = [idx for idx, _ in selected]
        
        weights = []
        for idx, score in selected:
            base_weight = 1.0 / (score + 1e-10)
            if self.current_round > self.params.bootstrap_rounds:
                # MORE aggressive reputation impact on weighting
                reputation_factor = self.reputation_scores[idx] ** 3.0  # Increased from 2.5
                weight = base_weight * reputation_factor
            else:
                weight = base_weight
            weights.append(weight)
        
        total_weight = sum(weights)
        weights = [w / total_weight for w in weights]
        
        # Update selection metrics and apply much stronger penalties for non-selection
        for idx in gradients_dict:
            was_selected = idx in selected_ids
            if not was_selected:
                # MUCH more aggressive suspicion score increase for non-selected
                self.participants[idx]['suspicion_score'] += 0.15  # Doubled from 0.08
                self.participants[idx]['rounds_since_selection'] += 1
            else:
                self.participants[idx]['selection_count'] += 1
                self.participants[idx]['rounds_since_selection'] = 0
                self.last_selected_round[idx] = self.current_round
            
            self.selection_history[idx].append(was_selected)
            self.selection_rates[idx] = self.participants[idx]['selection_count'] / max(1, self.current_round)
            
            # Early suspicion accumulation - flag all participants with high base scores
            if idx not in selected_ids and self.current_round <= self.params.bootstrap_rounds:
                if krum_scores[idx] > 1.5 * min(krum_scores.values()):
                    self.participants[idx]['suspicion_score'] += 0.1
                    self.suspicious_patterns[idx] += 1
        
        return selected_ids, weights
    
    def update_reputation_scores(self, selected_ids):
        for idx in self.participants:
            if not self.participants[idx]['active']:
                continue
                
            # Selected participants get reputation boost
            if idx in selected_ids:
                self.reputation_scores[idx] = min(1.0, self.reputation_scores[idx] + 0.07)
            else:
                # More aggressive reputation decay for non-selected participants
                self.reputation_scores[idx] = max(0.1, self.reputation_scores[idx] * self.params.reputation_decay)
            
            # Penalize for behavioral flags
            if self.participants[idx]['behavioral_flags'] >= self.params.behavioral_flag_threshold:
                penalty = self.params.reputation_penalty * (self.participants[idx]['behavioral_flags'] / self.params.behavioral_flag_threshold)
                self.reputation_scores[idx] = max(0.1, self.reputation_scores[idx] - penalty)
                self.participants[idx]['behavioral_flags'] = 0
            
            # Penalize for consistently low selection rates
            if self.current_round >= 10 and self.selection_rates[idx] < self.params.min_selection_rate:
                low_selection_penalty = 0.09 * (1.0 + (self.params.min_selection_rate - self.selection_rates[idx]) * 3)
                self.reputation_scores[idx] = max(0.1, self.reputation_scores[idx] - low_selection_penalty)
            
            # Reward for consistent selection
            if len(self.selection_history[idx]) >= 5:
                recent = self.selection_history[idx][-5:]
                if all(recent):  # Consistently selected
                    self.reputation_scores[idx] = min(1.0, self.reputation_scores[idx] + 0.06)
                elif not any(recent):  # Consistently NOT selected
                    self.reputation_scores[idx] = max(0.1, self.reputation_scores[idx] - 0.08)
            
            # Penalize suspicious patterns more aggressively
            if self.suspicious_patterns[idx] >= 4:  # Lowered threshold
                penalty = 0.08 * (self.suspicious_patterns[idx] / 4)
                self.reputation_scores[idx] = max(0.1, self.reputation_scores[idx] - penalty)
                self.suspicious_patterns[idx] = 0
        
        self._update_safe_list()
    
    def _update_safe_list(self):
        # Update safe list with more stringent criteria
        for idx in self.participants:
            if not self.participants[idx]['active']:
                continue
                
            # Add to safe list if meeting criteria
            if (self.selection_rates[idx] >= self.params.safe_list_selection_threshold and
                self.reputation_scores[idx] >= self.params.safe_list_reputation_threshold and
                self.current_round >= self.params.whitelist_rounds and
                self.gradient_cvs.get(idx, 1.0) < self.params.honest_gradient_cv_threshold):
                self.safe_list.add(idx)
                
            # Remove from safe list if reputation drops significantly
            if idx in self.safe_list and (
                self.reputation_scores[idx] < 0.5 or
                (self.current_round > 15 and self.selection_rates[idx] < 0.3) or
                self.gradient_cvs.get(idx, 0) > self.params.gradient_cv_threshold * 1.5):
                self.safe_list.remove(idx)
    
    def update_confidence_scores(self):
        active_ids = [idx for idx in self.participants if self.participants[idx]['active']]
        if not active_ids:
            return
            
        # Calculate environment metrics
        avg_gradient_cv = np.mean([self.gradient_cvs.get(idx, 0) for idx in active_ids])
        selection_rates = [self.selection_rates.get(idx, 0) for idx in active_ids]
        selection_variance = np.var(selection_rates) if len(selection_rates) > 1 else 0
        recent_accuracy = np.mean(self.accuracy_history[-5:]) if len(self.accuracy_history) >= 5 else 0.5
        confidence_values = [self.confidence_scores.get(idx, 0) for idx in active_ids]
        confidence_disparity = np.std(confidence_values) / (np.mean(confidence_values) + 1e-10) if len(active_ids) > 1 else 0
        
        # Update confidence scores for each participant
        for idx in self.participants:
            if not self.participants[idx]['active']:
                continue
                
            # Enhanced suspicion scoring with more weight on critical factors
            suspicion = min(self.participants[idx]['suspicion_score'] / self.params.removal_threshold, 1.0)
            
            # Selection-based factors with increased sensitivity
            selection_rate = self.selection_rates.get(idx, 0)
            selection_factor = max(0.0, 1.0 - selection_rate / 0.4)  # More sensitive threshold
            
            # Behavioral factors
            behavior_factor = min(self.participants[idx]['behavioral_flags'] / 4.0, 1.0)  # More sensitive threshold
            
            # Consecutive non-selection factor
            consecutive_factor = 0.0
            if len(self.selection_history[idx]) >= 4:  # Reduced window
                recent = self.selection_history[idx][-4:]
                if all(not x for x in recent):
                    consecutive_factor = 1.0
                elif sum(not x for x in recent) >= 3:  # More sensitive threshold
                    consecutive_factor = 0.8
            
            # Reputation factor
            reputation_factor = max(0.0, 1.0 - self.reputation_scores[idx])
            
            # Gradient variation factor
            gradient_cv_factor = min(self.gradient_cvs.get(idx, 0) / self.params.gradient_cv_threshold, 1.0)
            
            # Gradient deviation factor (from honest average)
            gradient_deviation_factor = min(self.gradient_deviations.get(idx, 0) * 1.5, 1.0)  # Scale up importance
            
            # Rounds since selection factor
            rounds_since_selection_factor = min(self.participants[idx]['rounds_since_selection'] / 5.0, 1.0)
            
            # Enhanced weighted confidence score calculation with more factors and higher weights
            weighted_confidence = (
                suspicion * self.params.suspicion_weight * 1.15 +  # Boosted for better recall
                selection_factor * self.params.selection_factor_weight +
                behavior_factor * self.params.behavior_factor_weight +
                consecutive_factor * self.params.consecutive_factor_weight +
                reputation_factor * self.params.reputation_factor_weight +
                gradient_cv_factor * 0.15 +  # Increased weight
                gradient_deviation_factor * 0.12 +  # New factor with higher weight
                rounds_since_selection_factor * 0.10  # New factor with higher weight
            )
            
            # Normalize and store confidence score
            self.confidence_scores[idx] = min(1.0, weighted_confidence)
            self.confidence_history[idx].append(self.confidence_scores[idx])
            self.historical_suspicion[idx].append(suspicion)
        
        # Adapt parameters based on environment assessment
        self.params.adapt_to_environment({
            'avg_gradient_cv': avg_gradient_cv,
            'selection_variance': selection_variance,
            'recent_accuracy': recent_accuracy,
            'confidence_disparity': confidence_disparity
        })
        self.parameter_history.append(self.params.get_status())
    
    def identify_honest_participants(self, val_loader=None):
        honest_candidates = []
        
        for idx in self.participants:
            if not self.participants[idx]['active']:
                continue
                
            # Selection rate criterion - must be frequently selected
            if self.selection_rates[idx] > self.params.honest_selection_rate_threshold:
                honest_candidates.append(idx)
                continue
                
            # Reputation criterion - must have high reputation
            if self.reputation_scores[idx] > self.params.honest_reputation_threshold:
                honest_candidates.append(idx)
                continue
                
            # Gradient variation criterion - must have stable gradients
            if idx in self.gradient_cvs and self.gradient_cvs[idx] < self.params.honest_gradient_cv_threshold:
                honest_candidates.append(idx)
                continue
                
            # Model accuracy criterion - must have good model performance
            if val_loader and len(self.participants[idx]['accuracy_history']) >= 3:
                avg_accuracy = np.mean(self.participants[idx]['accuracy_history'][-3:])
                if avg_accuracy > self.params.honest_accuracy_threshold:
                    honest_candidates.append(idx)
                    continue
                    
            # Safe list criterion - must be in the safe list
            if idx in self.safe_list:
                honest_candidates.append(idx)
                continue
                
            # Consistency criterion (new) - must have consistent behavior
            if idx in self.gradient_cosine_similarities and len(self.gradient_cosine_similarities[idx]) >= 3:
                similarities = self.gradient_cosine_similarities[idx][-3:]
                avg_similarity = np.mean(similarities)
                if avg_similarity > 0.2 and avg_similarity < 0.95:  # Reasonable range for honest participants
                    honest_candidates.append(idx)
                    continue
        
        return honest_candidates
    
    def detect_anomalies(self, honest_candidates, val_loader=None):
        # REMOVE THE BOOTSTRAP RESTRICTION - detect from round 1
        to_remove = []
        active_count = sum(1 for p in self.participants.values() if p['active'])
        max_removals = int(active_count * self.params.max_removal_rate)
        max_removals = min(max_removals, self.params.adv_per_round_limit)
        
        # Identify suspicious participants based on confidence scores - MUCH more aggressive
        candidates = []
        for idx in self.participants:
            if not self.participants[idx]['active'] or idx in honest_candidates or idx in self.safe_list:
                continue
                
            # MUCH LOWER confidence threshold check - almost always select someone
            if self.confidence_scores[idx] <= self.params.confidence_threshold * 0.7:  # Drastically reduced
                continue
                
            # Enhanced anomaly detection with multiple factors
            suspicion_score = self.confidence_scores[idx] * 1.5  # Base multiplier to increase ALL scores
            
            # Add bonus for ANY non-zero confidence value
            if self.confidence_scores[idx] > 0.1:
                suspicion_score += 0.2
            
            # Add LARGE bonus for gradient deviations
            if idx in self.gradient_deviations and self.gradient_deviations[idx] > 0.3:
                suspicion_score += self.gradient_deviations[idx] * 0.5  # Increased from 0.2
            
            # Add LARGE bonus for low selection rates
            if self.current_round >= 5 and self.selection_rates[idx] < 0.3:
                suspicion_score += (0.3 - self.selection_rates[idx]) * 0.6  # Doubled
            
            # Add LARGE bonus for high gradient CV
            if idx in self.gradient_cvs and self.gradient_cvs[idx] > self.params.gradient_cv_threshold:
                suspicion_score += (self.gradient_cvs[idx] - self.params.gradient_cv_threshold) * 0.6  # Doubled
            
            # Add LARGE bonus for consecutive non-selection
            if self.participants[idx]['rounds_since_selection'] >= 3:
                suspicion_score += min(self.participants[idx]['rounds_since_selection'] * 0.1, 0.3)
            
            candidates.append((idx, suspicion_score))
        
        # If no candidates met criteria, take the highest confidence scores anyway if above minimal threshold
        if not candidates and self.current_round >= 5:
            candidates = [(idx, self.confidence_scores[idx]) 
                          for idx in self.participants
                          if self.participants[idx]['active'] and idx not in honest_candidates 
                          and idx not in self.safe_list and self.confidence_scores[idx] > 0.1]
        
        # Sort by suspicion score and select top candidates for removal
        candidates.sort(key=lambda x: x[1], reverse=True)
        to_remove = [idx for idx, _ in candidates[:max_removals]]
        
        # ALWAYS remove at least one participant after bootstrap if we have candidates and accuracy is below threshold
        if not to_remove and self.current_round > self.params.bootstrap_rounds and candidates:
            if self.accuracy_history and self.accuracy_history[-1] < 0.6:
                to_remove = [candidates[0][0]]
        
        # Check how many honest participants would be removed
        honest_removed = sum(1 for idx in to_remove if self.participants[idx]['type'] == 'honest')
        
        # Apply honest protection budget
        if self.honest_removed_count + honest_removed > self.params.honest_removal_budget:
            # Only remove adversarial participants
            to_remove = [idx for idx in to_remove if self.participants[idx]['type'] != 'honest']
            honest_removed = 0
            
        self.honest_removed_count += honest_removed
        return to_remove, honest_removed
    
    def force_extreme_measures(self, honest_candidates, val_loader=None):
        # REMOVE BOOTSTRAP RESTRICTION - act from round 1
        if not self.params.forced_removal:
            return [], 0
            
        # Trigger extreme measures much more aggressively
        trigger_extreme_measures = False
        
        # Check if accuracy is below threshold or not improving significantly
        if self.accuracy_history and (self.accuracy_history[-1] < self.params.min_accuracy_threshold or
                                   (len(self.accuracy_history) >= 3 and
                                     self.accuracy_history[-1] < self.accuracy_history[-3] * 1.01)):
            trigger_extreme_measures = True
        
        # Also trigger every 2 rounds (was 3)
        if self.current_round % 2 == 0:
            trigger_extreme_measures = True
        
        # ALWAYS trigger in first few rounds
        if self.current_round <= 3:
            trigger_extreme_measures = True
            
        # If extreme measures triggered, implement MUCH more aggressive removal
        if trigger_extreme_measures:
            candidates = []
            for idx in self.participants:
                if not self.participants[idx]['active'] or idx in honest_candidates or idx in self.safe_list:
                    continue
                    
                # Calculate a comprehensive suspicion score - MUCH more aggressive
                suspicion = 0.0
                
                # Confidence score component (weighted MUCH more heavily)
                suspicion += self.confidence_scores[idx] * 2.0  # Increased from 1.5
                
                # Selection rate component
                if self.current_round >= 3:  # Reduced from 10
                    suspicion += (1.0 - self.selection_rates[idx]) * 1.2  # Increased from 0.8
                
                # Gradient deviation component
                if idx in self.gradient_deviations:
                    suspicion += self.gradient_deviations[idx] * 1.0  # Increased from 0.7
                
                # Gradient CV component
                if idx in self.gradient_cvs:
                    suspicion += min(self.gradient_cvs[idx] / (self.params.gradient_cv_threshold * 0.8), 1.0) * 0.8  # Increased sensitivity
                
                # Recent suspicion history component
                if idx in self.historical_suspicion and len(self.historical_suspicion[idx]) >= 2:  # Reduced from 3
                    suspicion += np.mean(self.historical_suspicion[idx][-2:]) * 0.8  # Increased from 0.5
                
                # Rounds since last selection component
                rounds_since = self.current_round - self.last_selected_round.get(idx, 0)
                suspicion += min(rounds_since / 3.0, 1.0) * 0.5  # Increased from 0.3
                
                # MUCH lower threshold for extreme measures
                if suspicion > 0.4:  # Reduced from 0.65
                    candidates.append((idx, suspicion))
                    
                # Special case: even if suspicion is low, always consider participants who are NEVER selected
                elif self.current_round >= 5 and self.selection_rates[idx] == 0:
                    candidates.append((idx, 0.4))  # Minimum viable score
            
            # ALWAYS remove at least one participant if we have candidates
            if candidates and self.params.adv_per_round_limit > 0:
                min_removals = min(1, len(candidates))  # At least 1 if we have any
                candidates.sort(key=lambda x: x[1], reverse=True)
                to_remove = [idx for idx, _ in candidates[:max(min_removals, self.params.adv_per_round_limit)]]
                
                # Check honest protection budget
                honest_removed = sum(1 for idx in to_remove if self.participants[idx]['type'] == 'honest')
                if self.honest_removed_count + honest_removed > self.params.honest_removal_budget:
                    to_remove = [idx for idx in to_remove if self.participants[idx]['type'] != 'honest']
                    honest_removed = 0
                    
                self.honest_removed_count += honest_removed
                return to_remove, honest_removed
            
        return [], 0
    
    def evaluate_model(self, val_loader):
        self.model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = self.model(inputs)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        return correct / total if total > 0 else 0.0
    
    def evaluate_participant_models(self, val_loader):
        if not val_loader:
            return
        for idx in self.participants:
            if not self.participants[idx]['active']:
                continue
            model = self.participants[idx]['model']
            model.eval()
            correct = 0
            total = 0
            with torch.no_grad():
                for inputs, labels in val_loader:
                    inputs, labels = inputs.to(self.device), labels.to(self.device)
                    outputs = model(inputs)
                    _, predicted = torch.max(outputs, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
            accuracy = correct / total if total > 0 else 0.0
            self.participants[idx]['accuracy_history'].append(accuracy)
    
    def update_model(self, gradients_dict, val_loader=None):
        selected_ids, weights = self.improved_multi_krum(gradients_dict)
        self.update_reputation_scores(selected_ids)
        self.update_confidence_scores()
        
        if not selected_ids:
            return 0.0
        
        for param in self.model.parameters():
            param.grad = None
        
        for idx, weight in zip(selected_ids, weights):
            for param, grad in zip(self.model.parameters(), gradients_dict[idx]):
                if param.grad is None:
                    param.grad = torch.zeros_like(param)
                param.grad += grad * weight
        
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        optimizer.step()
        
        accuracy = 0.0
        if val_loader:
            accuracy = self.evaluate_model(val_loader)
            self.accuracy_history.append(accuracy)
            if len(self.accuracy_history) >= 2:
                improvement = self.accuracy_history[-1] - self.accuracy_history[-2]
                self.accuracy_improvements.append(improvement)
            self.evaluate_participant_models(val_loader)
        
        return accuracy
    
    def check_early_stopping(self):
        if len(self.accuracy_history) < self.params.patience or self.current_round < self.params.min_rounds:
            return False
        recent = self.accuracy_history[-self.params.patience:]
        improvement = max(recent) - min(recent)
        return improvement < 0.01
    
    def get_metrics(self):
        active_honest = sum(1 for idx, p in self.participants.items() if p['active'] and p['type'] == 'honest')
        active_adversarial = sum(1 for idx, p in self.participants.items() if p['active'] and p['type'] == 'adversarial')
        honest_selection_rates = [self.selection_rates.get(idx, 0) 
                                 for idx, p in self.participants.items() 
                                 if p['type'] == 'honest' and p['active']]
        adversarial_selection_rates = [self.selection_rates.get(idx, 0) 
                                      for idx, p in self.participants.items() 
                                      if p['type'] == 'adversarial' and p['active']]
        avg_honest_selection = np.mean(honest_selection_rates) if honest_selection_rates else 0
        avg_adversarial_selection = np.mean(adversarial_selection_rates) if adversarial_selection_rates else 0
        honest_confidence = [self.confidence_scores.get(idx, 0) 
                            for idx, p in self.participants.items() 
                            if p['type'] == 'honest' and p['active']]
        adversarial_confidence = [self.confidence_scores.get(idx, 0) 
                                 for idx, p in self.participants.items() 
                                 if p['type'] == 'adversarial' and p['active']]
        avg_honest_confidence = np.mean(honest_confidence) if honest_confidence else 0
        avg_adversarial_confidence = np.mean(adversarial_confidence) if adversarial_confidence else 0
        return {
            'active_honest': active_honest,
            'active_adversarial': active_adversarial,
            'total_active': active_honest + active_adversarial,
            'avg_honest_selection': avg_honest_selection,
            'avg_adversarial_selection': avg_adversarial_selection,
            'selection_gap': avg_honest_selection - avg_adversarial_selection,
            'avg_honest_confidence': avg_honest_confidence,
            'avg_adversarial_confidence': avg_adversarial_confidence,
            'confidence_gap': avg_adversarial_confidence - avg_honest_confidence,
            'accuracy': self.accuracy_history[-1] if self.accuracy_history else 0,
            'current_params': self.params.get_status(),
            'honest_removed': self.honest_removed_count,
            'safe_list_size': len(self.safe_list)
        }

# Improved Participant Training
def train_improved_participant(participant, dataset, device, current_round, val_loader=None, server=None):
    model = participant['model']
    model.train()
    
    # Dynamic learning rate schedule
    lr = 0.001
    if current_round < 5:
        lr = 0.001 * (0.5 + 0.1 * current_round)
    else:
        lr = 0.001 * (0.98 ** ((current_round - 5) // 5))
    
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = FocalLoss(alpha=0.85, gamma=2.5)  # Enhanced focal loss parameters
    
    data_loader = participant['data']
    if not data_loader:
        return [], 0.0, 0.0
    
    total_loss = 0.0
    if server and server.params:
        adversary_proportion = server.params.current_detected_ratio
        noise_scale = 0.05 + adversary_proportion * 0.12
        flip_prob = 0.25 + adversary_proportion * 0.35
    else:
        adversary_proportion = 0.5
        noise_scale = 0.12
        flip_prob = 0.3
    
    training_epochs = 6  # Reduced to avoid overfitting
    for epoch in range(training_epochs):
        participant['epoch'] = participant.get('epoch', 0) + 1
        epoch_loss = 0.0
        for features, labels in data_loader:
            features, labels = features.to(device), labels.to(device)
            
            # Adversarial behavior simulation with more sophisticated strategies
            if participant['type'] == 'adversarial':
                current_epoch = participant.get('epoch', 0)
                selection_rate = participant.get('selection_count', 0) / max(current_round, 1)
                
                # Adaptive adversarial strategy based on current detection environment
                if current_epoch < 10:
                    # Early stage: more aggressive attacks
                    if random.random() < flip_prob:
                        labels = (labels + 1) % dataset.num_classes
                    if random.random() < 0.65:  # Increased noise probability
                        noise = torch.randn_like(features) * noise_scale
                        features = features + noise
                        
                elif current_epoch < 30:
                    # Middle stage: adapt based on selection rate
                    if selection_rate < 0.3:  # If rarely selected, be more stealthy
                        if random.random() < flip_prob * 0.5:  # Less aggressive label flipping
                            labels = (labels + 1) % dataset.num_classes
                        if random.random() < 0.3:  # Less aggressive noise
                            noise = torch.randn_like(features) * noise_scale * 0.6
                            features = features + noise
                    else:  # If regularly selected, maintain attack intensity
                        if random.random() < flip_prob * 0.8:
                            labels = (labels + 1) % dataset.num_classes
                        if random.random() < 0.5:
                            noise = torch.randn_like(features) * noise_scale
                            features = features + noise
                            
                else:
                    # Late stage: highly adaptive strategy
                    if selection_rate < 0.2:  # Very rarely selected - be very stealthy
                        if random.random() < flip_prob * 0.2:  # Minimal label flipping
                            labels = (labels + 1) % dataset.num_classes
                        if random.random() < 0.2:  # Minimal noise
                            noise = torch.randn_like(features) * noise_scale * 0.3
                            features = features + noise
                    elif selection_rate < 0.4:  # Sometimes selected - moderate stealth
                        if random.random() < flip_prob * 0.4:
                            labels = (labels + 1) % dataset.num_classes
                        if random.random() < 0.3:
                            noise = torch.randn_like(features) * noise_scale * 0.7
                            features = features + noise
                    else:  # Frequently selected - can be more aggressive
                        if random.random() < flip_prob * 0.7:
                            labels = (labels + 1) % dataset.num_classes
                        if random.random() < 0.5:
                            noise = torch.randn_like(features) * noise_scale * 0.9
                            features = features + noise
                
                # Additional strategic behaviors based on detection environment
                if server and server.params:
                    if server.params.current_detected_ratio > 0.7:  # High detection environment
                        # Be more stealthy in high-detection environments
                        if random.random() < 0.6:  # 60% chance to be stealthy
                            features = features  # No attack
                            # Maybe inject a small amount of targeted noise
                            if random.random() < 0.3:
                                specific_features = random.sample(range(features.shape[1]), 2)
                                for feat_idx in specific_features:
                                    features[:, feat_idx] *= (1 + random.uniform(-0.1, 0.1))
                    elif server.params.current_detected_ratio < 0.3:  # Low detection environment
                        # Be more aggressive in low-detection environments
                        if random.random() < 0.7:  # 70% chance to attack
                            labels = (labels + 1) % dataset.num_classes
                            if random.random() < 0.6:
                                noise = torch.randn_like(features) * noise_scale * 1.2
                                features = features + noise
            
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()
            epoch_loss += loss.item()
        total_loss += epoch_loss / len(data_loader)
    
    gradients = [param.grad.clone().detach() for param in model.parameters()]
    accuracy = 0.0
    if val_loader:
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        accuracy = correct / total if total > 0 else 0.0
    
    return gradients, total_loss / training_epochs, accuracy

# Improved Data Distribution
def distribute_data_to_participants(server, dataset, train_indices):
    activated_honest = 0
    activated_adversarial = 0
    record_counts = defaultdict(int)
    
    for idx in train_indices:
        if idx >= len(dataset.participant_ids):
            continue  # Skip synthetic indices
        pid = dataset.participant_ids[idx]
        record_counts[pid] += 1
    
    print("Participant record distribution:")
    for pid, count in sorted(record_counts.items()):
        print(f"Participant {pid}: {count} records")
    
    for idx in server.participants:
        # Use original indices for this participant
        indices = [i for i in train_indices if i < len(dataset.participant_ids) and dataset.participant_ids[i] == idx]
        num_records = len(indices)
        if num_records < 5:
            print(f"Warning: Participant {idx} has only {num_records} records, deactivating")
            server.participants[idx]['active'] = False
            continue
        if idx not in dataset.participant_indices:
            print(f"Warning: Participant {idx} not found in dataset, deactivating")
            server.participants[idx]['active'] = False
            continue
        # Create subset with resampled data, but only for original indices
        subset = Subset(dataset, indices)
        batch_size = min(32, max(4, num_records // 2))
        server.participants[idx]['data'] = DataLoader(subset, batch_size=batch_size, shuffle=True)
        server.participants[idx]['active'] = True
        if server.participants[idx]['type'] == 'honest':
            activated_honest += 1
        else:
            activated_adversarial += 1
    
    print(f"Activated {activated_honest} honest and {activated_adversarial} adversarial participants")
    actual_honest_ratio = activated_honest / max(1, activated_honest + activated_adversarial)
    if actual_honest_ratio < 0.4:
        print(f"Warning: Very low honest ratio ({actual_honest_ratio:.2f}), adjusting parameters")
        server.params.update_parameters(0.8)
    elif actual_honest_ratio > 0.8:
        print(f"Warning: Very high honest ratio ({actual_honest_ratio:.2f}), adjusting parameters")
        server.params.update_parameters(0.2)
    
    return activated_honest, activated_adversarial

# Improved Evaluation Metrics
def calculate_improved_metrics(server, removed_participants):
    y_true = [1 if server.participants[idx]['type'] == 'adversarial' else 0 for idx in server.participants]
    y_pred = [1 if idx in removed_participants else 0 for idx in server.participants]
    
    if sum(y_pred) == 0:
        precision = 1.0
        recall = 0.0
        f1 = 0.0
        accuracy = y_true.count(0) / len(y_true)
    else:
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=1.0)
        recall = recall_score(y_true, y_pred, zero_division=0.0)
        f1 = f1_score(y_true, y_pred, zero_division=0.0)
    
    honest_removed = sum(1 for idx in removed_participants if server.participants[idx]['type'] == 'honest')
    adversarial_removed = sum(1 for idx in removed_participants if server.participants[idx]['type'] == 'adversarial')
    total_honest = sum(1 for p in server.participants.values() if p['type'] == 'honest')
    total_adversarial = sum(1 for p in server.participants.values() if p['type'] == 'adversarial')
    
    honest_removal_rate = honest_removed / max(1, total_honest)
    adversarial_removal_rate = adversarial_removed / max(1, total_adversarial)
    removal_preference = adversarial_removal_rate / honest_removal_rate if honest_removal_rate > 0 else float('inf')
    
    tp = sum(1 for idx in removed_participants if server.participants[idx]['type'] == 'adversarial')
    fp = sum(1 for idx in removed_participants if server.participants[idx]['type'] == 'honest')
    tn = sum(1 for idx in server.participants if server.participants[idx]['type'] == 'honest' and idx not in removed_participants)
    fn = sum(1 for idx in server.participants if server.participants[idx]['type'] == 'adversarial' and idx not in removed_participants)
    
    model_accuracy = np.mean(server.accuracy_history[-5:]) if len(server.accuracy_history) >= 5 else (server.accuracy_history[-1] if server.accuracy_history else 0.0)
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 1.0
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0
    
    roc_auc = 0.0
    if len(set(y_true)) > 1 and len(set(y_pred)) > 1:
        confidence_scores = [server.confidence_scores.get(idx, 0.0) for idx in server.participants]
        try:
            roc_auc = roc_auc_score(y_true, confidence_scores)
        except:
            roc_auc = 0.0
    
    initial_params = server.parameter_history[0] if server.parameter_history else {}
    final_params = server.parameter_history[-1] if server.parameter_history else {}
    removal_threshold_change = final_params.get('removal_threshold', 0) - initial_params.get('removal_threshold', 0)
    confidence_threshold_change = final_params.get('confidence_threshold', 0) - initial_params.get('confidence_threshold', 0)
    detection_ratio_change = final_params.get('detected_ratio', 0.5) - initial_params.get('detected_ratio', 0.5)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'specificity': specificity,
        'npv': npv,
        'roc_auc': roc_auc,
        'honest_removed': honest_removed,
        'adversarial_removed': adversarial_removed,
        'honest_removal_rate': honest_removal_rate,
        'adversarial_removal_rate': adversarial_removal_rate,
        'removal_preference': removal_preference,
        'tp': tp,
        'fp': fp,
        'tn': tn,
        'fn': fn,
        'model_accuracy': model_accuracy,
        'final_accuracy': server.accuracy_history[-1] if server.accuracy_history else 0.0,
        'max_accuracy': max(server.accuracy_history) if server.accuracy_history else 0.0,
        'removal_threshold_change': removal_threshold_change,
        'confidence_threshold_change': confidence_threshold_change,
        'detection_ratio_change': detection_ratio_change,
        'final_detection_ratio': final_params.get('detected_ratio', 0.5),
        'safe_list_count': len(server.safe_list),
        'rounds_completed': server.current_round
    }

def print_detection_performance(metrics):
    print("\nDETECTION PERFORMANCE:")
    print("-" * 50)
    print(f"Accuracy:      {metrics['accuracy']*100:.2f}%")
    print(f"Precision:     {metrics['precision']*100:.2f}%")
    print(f"Recall:        {metrics['recall']*100:.2f}%")
    print(f"F1 Score:      {metrics['f1']*100:.2f}%")
    print(f"ROC AUC:       {metrics['roc_auc']*100:.2f}%")
    print(f"Model Acc (last 5): {metrics['model_accuracy']*100:.2f}%")
    print("-" * 50)
    print(f"Honest Removed:       {metrics['honest_removal_rate']*100:.1f}% ({metrics['honest_removed']}/{metrics['honest_removed']+metrics['tn']})")
    print(f"Adversarial Removed:  {metrics['adversarial_removal_rate']*100:.1f}% ({metrics['adversarial_removed']}/{metrics['adversarial_removed']+metrics['fn']})")
    print(f"Removal Preference:   {metrics['removal_preference']:.1f}x" if metrics['removal_preference'] != float('inf') else "Removal Preference: infx")
    print("-" * 50)
    print(f"True Positives:      {metrics['tp']}")
    print(f"False Positives:     {metrics['fp']}")
    print(f"True Negatives:      {metrics['tn']}")
    print(f"False Negatives:     {metrics['fn']}")
    print("-" * 50)
    print(f"Parameter Adaptation:")
    print(f"  Final Detection Ratio: {metrics['final_detection_ratio']:.2f}")
    print(f"  Removal Threshold Change: {metrics['removal_threshold_change']:.2f}")
    print(f"  Confidence Threshold Change: {metrics['confidence_threshold_change']:.2f}")
    print("-" * 50)

def create_performance_plots(server, metrics, save_dir='.'):
    os.makedirs(save_dir, exist_ok=True)
    plt.figure(figsize=(12, 8))
    plt.subplot(2, 2, 1)
    plt.plot(server.accuracy_history, label='Model Accuracy')
    plt.xlabel('Round')
    plt.ylabel('Accuracy')
    plt.title('Model Accuracy Over Training')
    plt.grid(True)
    plt.legend()
    
    plt.subplot(2, 2, 2)
    honest_confidence = []
    adversarial_confidence = []
    for round_idx in range(server.current_round):
        round_honest = []
        round_adversarial = []
        for idx in server.participants:
            if idx in server.confidence_history and len(server.confidence_history[idx]) > round_idx:
                if server.participants[idx]['type'] == 'honest':
                    round_honest.append(server.confidence_history[idx][round_idx])
                else:
                    round_adversarial.append(server.confidence_history[idx][round_idx])
        honest_confidence.append(np.mean(round_honest) if round_honest else 0)
        adversarial_confidence.append(np.mean(round_adversarial) if round_adversarial else 0)
    
    rounds = range(len(honest_confidence))
    plt.plot(rounds, honest_confidence, 'g-', label='Honest')
    plt.plot(rounds, adversarial_confidence, 'r-', label='Adversarial')
    plt.xlabel('Round')
    plt.ylabel('Avg Confidence Score')
    plt.title('Confidence Score Evolution')
    plt.grid(True)
    plt.legend()
    
    plt.subplot(2, 2, 3)
    if server.parameter_history:
        rounds = range(len(server.parameter_history))
        detected_ratios = [p['detected_ratio'] for p in server.parameter_history]
        removal_thresholds = [p['removal_threshold'] for p in server.parameter_history]
        confidence_thresholds = [p['confidence_threshold'] for p in server.parameter_history]
        plt.plot(rounds, detected_ratios, 'b-', label='Detection Ratio')
        plt.plot(rounds, removal_thresholds, 'g--', label='Removal Threshold')
        plt.plot(rounds, confidence_thresholds, 'r-.', label='Confidence Threshold')
        plt.xlabel('Round')
        plt.ylabel('Parameter Value')
        plt.title('Parameter Adaptation Over Time')
        plt.grid(True)
        plt.legend()
    
    plt.subplot(2, 2, 4)
    active_honest = []
    active_adversarial = []
    for round_idx in range(1, server.current_round + 1):
        honest_count = 0
        adversarial_count = 0
        for idx, participant in server.participants.items():
            if idx not in server.removed_participants[:round_idx]:
                if participant['type'] == 'honest':
                    honest_count += 1
                else:
                    adversarial_count += 1
        active_honest.append(honest_count)
        active_adversarial.append(adversarial_count)
    
    rounds = range(1, len(active_honest) + 1)
    plt.plot(rounds, active_honest, 'g-', label='Active Honest')
    plt.plot(rounds, active_adversarial, 'r-', label='Active Adversarial')
    plt.xlabel('Round')
    plt.ylabel('Count')
    plt.title('Active Participants Over Time')
    plt.grid(True)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/performance_plots.png')
    plt.close()
    
    plt.figure(figsize=(10, 8))
    cm = np.array([[metrics['tn'], metrics['fp']], [metrics['fn'], metrics['tp']]])
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    classes = ['Honest', 'Adversarial']
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes)
    plt.yticks(tick_marks, classes)
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'), ha="center", va="center", color="white" if cm[i, j] > thresh else "black")
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(f'{save_dir}/confusion_matrix.png')
    plt.close()

# Improved Simulation
def run_improved_v2g_simulation(dataset_path, num_honest, num_adversarial, rounds=100, device='cpu', adaptive_mode=True, save_dir='.', no_early_stop=False):
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    
    print(f"Loading dataset from {dataset_path}...")
    dataset = V2GDataset(dataset_path)
    
    model = V2GClassifier(input_size=dataset.num_features, num_classes=dataset.num_classes).to(device)
    server = ImprovedFederatedServer(model, num_honest, num_adversarial, device, dataset, adaptive_mode)
    
    # Use original indices (before SMOTE)
    indices = list(dataset.get_original_indices())
    random.shuffle(indices)
    train_size = int(0.8 * len(indices))
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    val_dataset = Subset(dataset, val_indices)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    
    print("Distributing data to participants...")
    activated_honest, activated_adversarial = distribute_data_to_participants(server, dataset, train_indices)
    
    total_active = activated_honest + activated_adversarial
    if total_active < server.params.multi_krum_m:
        print(f"Warning: Only {total_active} active participants, adjusting Multi-Krum parameters")
        server.params.multi_krum_m = max(3, total_active // 2)
        server.params.num_byzantines = max(1, min(5, activated_adversarial // 2))
        print(f"Adjusted Multi-Krum params: byzantines={server.params.num_byzantines}, m={server.params.multi_krum_m}")
    
    print(f"Starting FL with {activated_honest} honest and {activated_adversarial} adversarial participants")
    print(f"Bootstrap rounds: {server.params.bootstrap_rounds}, Whitelist rounds: {server.params.whitelist_rounds}")
    print(f"Removal threshold: {server.params.removal_threshold:.2f}, Max removal rate: {server.params.max_removal_rate*100:.1f}%")
    print(f"Protection strategy: Limit honest removals to {server.params.honest_removal_budget} participants")
    print(f"Adaptive mode: {'ON' if adaptive_mode else 'OFF'}")
    
    removed_participants = []
    early_stopped = False
    
    for round_idx in range(rounds):
        server.current_round += 1
        print(f"\n=== Round {server.current_round} ===")
        gradients_dict = {}
        
        for idx in server.participants:
            if not server.participants[idx]['active']:
                continue
            gradients, loss, accuracy = train_improved_participant(
                server.participants[idx], dataset, device, server.current_round, val_loader, server
            )
            if gradients:
                gradients_dict[idx] = gradients
                server.calculate_gradient_metrics(idx, gradients)
        
        accuracy = server.update_model(gradients_dict, val_loader)
        selected_ids, _ = server.improved_multi_krum(gradients_dict)
        honest_selected = sum(1 for idx in selected_ids if server.participants[idx]['type'] == 'honest')
        adversarial_selected = len(selected_ids) - honest_selected
        print(f"Multi-Krum selection: {honest_selected} honest, {adversarial_selected} adversarial (total: {len(selected_ids)})")
        
        # Regular adversarial detection every round starting from round 1
        honest_candidates = server.identify_honest_participants(val_loader)
        to_remove, honest_removed = server.detect_anomalies(honest_candidates, val_loader)
        
        # Force extreme measures every 2 rounds and always in early rounds
        if server.current_round % 2 == 0 or server.current_round <= 3 or (
            len(server.accuracy_history) >= 2 and server.accuracy_history[-1] < server.accuracy_history[-2]):
            print(f"TRIGGER: {'Early detection' if server.current_round <= 3 else 'Scheduled check' if server.current_round % 2 == 0 else 'Accuracy drop'}")
            extreme_remove, extreme_honest = server.force_extreme_measures(honest_candidates, val_loader)
            
            # If extreme measures found anything, use those results instead
            if extreme_remove:
                print(f"EXTREME MEASURES removed {len(extreme_remove)} participants (Honest: {extreme_honest}, "
                     f"Adversarial: {len(extreme_remove) - extreme_honest})")
                to_remove = extreme_remove
                honest_removed = extreme_honest
            print(f"TRIGGER: {'Scheduled adversarial check' if server.current_round % 4 == 0 else 'Accuracy drop detected'}")
            extreme_remove, extreme_honest = server.force_extreme_measures(honest_candidates, val_loader)
            
            # If extreme measures found anything, use those results instead
            if extreme_remove:
                print(f"EXTREME MEASURES removed {len(extreme_remove)} participants (Honest: {extreme_honest}, "
                     f"Adversarial: {len(extreme_remove) - extreme_honest})")
                to_remove = extreme_remove
                honest_removed = extreme_honest
        
        if to_remove:
            for idx in to_remove:
                server.participants[idx]['active'] = False
                removed_participants.append(idx)
                server.removed_participants.append(idx)
            
            adv_removed = len(to_remove) - honest_removed
            print(f"Removed {len(to_remove)} participants (Honest: {honest_removed}, Adversarial: {adv_removed})")
            if honest_removed == 0 and len(to_remove) > 0:
                print("  Perfect removal! Only adversarial participants removed.")
            print(f"  Current model accuracy: {accuracy:.2f}")
        
        metrics = server.get_metrics()
        print(f"Active: {metrics['total_active']} (Honest: {metrics['active_honest']}, Adversarial: {metrics['active_adversarial']})")
        print(f"Accuracy: {accuracy:.2f}, Selection Gap: {metrics['selection_gap']:.2f}, Confidence Gap: {metrics['confidence_gap']:.2f}")
        if adaptive_mode:
            print(f"Adaptive Parameters: Detection Ratio={metrics['current_params']['detected_ratio']:.2f}, " +
                 f"Removal Threshold={metrics['current_params']['removal_threshold']:.2f}, " +
                 f"Confidence Threshold={metrics['current_params']['confidence_threshold']:.2f}")
        
        if not no_early_stop and server.check_early_stopping():
            print(f"Early stopping triggered at round {server.current_round} - validation accuracy not improving")
            early_stopped = True
            break
        
        if metrics['total_active'] < server.params.multi_krum_m:
            print(f"Too few active participants ({metrics['total_active']}) at round {server.current_round}, stopping")
            break
    
    print("\nCreating performance visualizations...")
    metrics = calculate_improved_metrics(server, removed_participants)
    create_performance_plots(server, metrics, save_dir)
    
    print_detection_performance(metrics)
    
    honest_selection_rates = [server.selection_rates.get(idx, 0) 
                             for idx, p in server.participants.items() 
                             if p['type'] == 'honest']
    adversarial_selection_rates = [server.selection_rates.get(idx, 0) 
                                  for idx, p in server.participants.items() 
                                  if p['type'] == 'adversarial']
    
    print("\nPARTICIPANT BEHAVIOR ANALYSIS:")
    print("-" * 50)
    print(f"Average selection rate (honest):      {np.mean(honest_selection_rates):.3f}")
    print(f"Average selection rate (adversarial): {np.mean(adversarial_selection_rates):.3f}")
    print(f"Selection rate gap:                   {np.mean(honest_selection_rates) - np.mean(adversarial_selection_rates):.3f}")
    
    print("\nFINAL RESULTS:")
    print(f"Honest participants: {metrics['tn']}/{metrics['tn'] + metrics['fp']} remaining " +
          f"({metrics['honest_removal_rate']*100:.1f}% removed)")
    print(f"Adversarial participants: {metrics['fn']}/{metrics['fn'] + metrics['tp']} remaining " +
          f"({metrics['adversarial_removal_rate']*100:.1f}% removed)")
    print(f"Final model accuracy: {metrics['model_accuracy']:.2f}")
    print(f"Max model accuracy achieved: {metrics['max_accuracy']:.2f}")
    print(f"Removal preference ratio: {metrics['removal_preference']:.1f}x" 
          if metrics['removal_preference'] != float('inf') else "Removal preference ratio: infx")
    
    print("\nSimulation completed!")
    print(f"Total rounds: {server.current_round}/{rounds}" + (" (early stopped)" if early_stopped else ""))
    print(f"Detection F1 score: {metrics['f1']*100:.2f}%")
    print(f"Recall: {metrics['recall']*100:.2f}%")
    print(f"Relative preference for removing adversaries: {metrics['removal_preference']:.1f}x" 
          if metrics['removal_preference'] != float('inf') else "Relative preference for removing adversaries: infx")
    print(f"Honest participants removed: {metrics['honest_removal_rate']*100:.1f}%")
    print(f"Adversarial participants removed: {metrics['adversarial_removal_rate']*100:.1f}%")
    
    return server, removed_participants, metrics

# Main execution
if __name__ == "__main__":
    device = 'cpu'
    print(f"Using device: {device}")
    dataset_path = r"C:\Users\Administrator\Downloads\v2g_simulated_dataset (1).csv"  # Update with your dataset path
    os.makedirs('./honest_majority_results', exist_ok=True)
    os.makedirs('./adversarial_majority_results', exist_ok=True)
    
    print("\n=== RUNNING HONEST MAJORITY SCENARIO ===")
    num_honest = 45
    num_adversarial = 44
    server_honest, removed_honest, metrics_honest = run_improved_v2g_simulation(
        dataset_path, num_honest, num_adversarial, rounds=100, device=device, 
        adaptive_mode=True, save_dir='./honest_majority_results'
    )
    
    print("\n\n=== RUNNING ADVERSARIAL MAJORITY SCENARIO ===")
    num_honest = 45
    num_adversarial = 46
    server_adv, removed_adv, metrics_adv = run_improved_v2g_simulation(
        dataset_path, num_honest, num_adversarial, rounds=100, device=device, 
        adaptive_mode=True, save_dir='./adversarial_majority_results'
    )
    
    print("\n\n=== SCENARIO COMPARISON ===")
    print("-" * 50)
    print(f"Metric | Honest Majority | Adversarial Majority")
    print("-" * 50)
    print(f"F1 Score | {metrics_honest['f1']*100:.1f}% | {metrics_adv['f1']*100:.1f}%")
    print(f"Recall | {metrics_honest['recall']*100:.1f}% | {metrics_adv['recall']*100:.1f}%")
    print(f"Adv Removed | {metrics_honest['adversarial_removal_rate']*100:.1f}% | {metrics_adv['adversarial_removal_rate']*100:.1f}%")
    print(f"Honest Removed | {metrics_honest['honest_removal_rate']*100:.1f}% | {metrics_adv['honest_removal_rate']*100:.1f}%")
    print(f"Model Accuracy | {metrics_honest['model_accuracy']*100:.1f}% | {metrics_adv['model_accuracy']*100:.1f}%")
    print(f"Rounds | {server_honest.current_round} | {server_adv.current_round}")
    print(f"Final Detection Ratio | {metrics_honest['final_detection_ratio']:.2f} | {metrics_adv['final_detection_ratio']:.2f}")
    print("-" * 50)